{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e341qJR7kHmv",
        "outputId": "05ef9173-2154-4edc-95c9-d1583250cef3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "GLLXIEzinICg",
        "outputId": "c6cdb0fc-c5b3-44a4-8a64-018a79c8fdec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0    842302         M        17.99         10.38          122.80     1001.0   \n",
              "1    842517         M        20.57         17.77          132.90     1326.0   \n",
              "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
              "3  84348301         M        11.42         20.38           77.58      386.1   \n",
              "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
              "\n",
              "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0          0.11840           0.27760          0.3001              0.14710   \n",
              "1          0.08474           0.07864          0.0869              0.07017   \n",
              "2          0.10960           0.15990          0.1974              0.12790   \n",
              "3          0.14250           0.28390          0.2414              0.10520   \n",
              "4          0.10030           0.13280          0.1980              0.10430   \n",
              "\n",
              "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
              "0  ...          17.33           184.60      2019.0            0.1622   \n",
              "1  ...          23.41           158.80      1956.0            0.1238   \n",
              "2  ...          25.53           152.50      1709.0            0.1444   \n",
              "3  ...          26.50            98.87       567.7            0.2098   \n",
              "4  ...          16.67           152.20      1575.0            0.1374   \n",
              "\n",
              "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
              "0             0.6656           0.7119                0.2654          0.4601   \n",
              "1             0.1866           0.2416                0.1860          0.2750   \n",
              "2             0.4245           0.4504                0.2430          0.3613   \n",
              "3             0.8663           0.6869                0.2575          0.6638   \n",
              "4             0.2050           0.4000                0.1625          0.2364   \n",
              "\n",
              "   fractal_dimension_worst  Unnamed: 32  \n",
              "0                  0.11890          NaN  \n",
              "1                  0.08902          NaN  \n",
              "2                  0.08758          NaN  \n",
              "3                  0.17300          NaN  \n",
              "4                  0.07678          NaN  \n",
              "\n",
              "[5 rows x 33 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5992729c-3905-4cdb-b214-2a984f0c82af\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 33 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5992729c-3905-4cdb-b214-2a984f0c82af')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5992729c-3905-4cdb-b214-2a984f0c82af button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5992729c-3905-4cdb-b214-2a984f0c82af');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9fadf42e-0eb9-48cd-8a89-e233ebbfac99\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9fadf42e-0eb9-48cd-8a89-e233ebbfac99')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9fadf42e-0eb9-48cd-8a89-e233ebbfac99 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()\n",
        "# The upper Headings are said to be the features used for this Cancer Detection project."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "LFXuCdWunJ-F",
        "outputId": "9f528a27-5d15-4989-9116-7c0c611fa962"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
              "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
              "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
              "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
              "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
              "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
              "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
              "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
              "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
              "\n",
              "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "count       569.000000        569.000000      569.000000           569.000000   \n",
              "mean          0.096360          0.104341        0.088799             0.048919   \n",
              "std           0.014064          0.052813        0.079720             0.038803   \n",
              "min           0.052630          0.019380        0.000000             0.000000   \n",
              "25%           0.086370          0.064920        0.029560             0.020310   \n",
              "50%           0.095870          0.092630        0.061540             0.033500   \n",
              "75%           0.105300          0.130400        0.130700             0.074000   \n",
              "max           0.163400          0.345400        0.426800             0.201200   \n",
              "\n",
              "       symmetry_mean  ...  texture_worst  perimeter_worst   area_worst  \\\n",
              "count     569.000000  ...     569.000000       569.000000   569.000000   \n",
              "mean        0.181162  ...      25.677223       107.261213   880.583128   \n",
              "std         0.027414  ...       6.146258        33.602542   569.356993   \n",
              "min         0.106000  ...      12.020000        50.410000   185.200000   \n",
              "25%         0.161900  ...      21.080000        84.110000   515.300000   \n",
              "50%         0.179200  ...      25.410000        97.660000   686.500000   \n",
              "75%         0.195700  ...      29.720000       125.400000  1084.000000   \n",
              "max         0.304000  ...      49.540000       251.200000  4254.000000   \n",
              "\n",
              "       smoothness_worst  compactness_worst  concavity_worst  \\\n",
              "count        569.000000         569.000000       569.000000   \n",
              "mean           0.132369           0.254265         0.272188   \n",
              "std            0.022832           0.157336         0.208624   \n",
              "min            0.071170           0.027290         0.000000   \n",
              "25%            0.116600           0.147200         0.114500   \n",
              "50%            0.131300           0.211900         0.226700   \n",
              "75%            0.146000           0.339100         0.382900   \n",
              "max            0.222600           1.058000         1.252000   \n",
              "\n",
              "       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
              "count            569.000000      569.000000               569.000000   \n",
              "mean               0.114606        0.290076                 0.083946   \n",
              "std                0.065732        0.061867                 0.018061   \n",
              "min                0.000000        0.156500                 0.055040   \n",
              "25%                0.064930        0.250400                 0.071460   \n",
              "50%                0.099930        0.282200                 0.080040   \n",
              "75%                0.161400        0.317900                 0.092080   \n",
              "max                0.291000        0.663800                 0.207500   \n",
              "\n",
              "       Unnamed: 32  \n",
              "count          0.0  \n",
              "mean           NaN  \n",
              "std            NaN  \n",
              "min            NaN  \n",
              "25%            NaN  \n",
              "50%            NaN  \n",
              "75%            NaN  \n",
              "max            NaN  \n",
              "\n",
              "[8 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9b11f137-4109-40c0-aa6e-c20de4216487\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.690000e+02</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.037183e+07</td>\n",
              "      <td>14.127292</td>\n",
              "      <td>19.289649</td>\n",
              "      <td>91.969033</td>\n",
              "      <td>654.889104</td>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.181162</td>\n",
              "      <td>...</td>\n",
              "      <td>25.677223</td>\n",
              "      <td>107.261213</td>\n",
              "      <td>880.583128</td>\n",
              "      <td>0.132369</td>\n",
              "      <td>0.254265</td>\n",
              "      <td>0.272188</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.290076</td>\n",
              "      <td>0.083946</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.250206e+08</td>\n",
              "      <td>3.524049</td>\n",
              "      <td>4.301036</td>\n",
              "      <td>24.298981</td>\n",
              "      <td>351.914129</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>0.052813</td>\n",
              "      <td>0.079720</td>\n",
              "      <td>0.038803</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>...</td>\n",
              "      <td>6.146258</td>\n",
              "      <td>33.602542</td>\n",
              "      <td>569.356993</td>\n",
              "      <td>0.022832</td>\n",
              "      <td>0.157336</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.065732</td>\n",
              "      <td>0.061867</td>\n",
              "      <td>0.018061</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>8.670000e+03</td>\n",
              "      <td>6.981000</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>...</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.055040</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.692180e+05</td>\n",
              "      <td>11.700000</td>\n",
              "      <td>16.170000</td>\n",
              "      <td>75.170000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>0.086370</td>\n",
              "      <td>0.064920</td>\n",
              "      <td>0.029560</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>...</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>84.110000</td>\n",
              "      <td>515.300000</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.114500</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>0.071460</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>9.060240e+05</td>\n",
              "      <td>13.370000</td>\n",
              "      <td>18.840000</td>\n",
              "      <td>86.240000</td>\n",
              "      <td>551.100000</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.092630</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>...</td>\n",
              "      <td>25.410000</td>\n",
              "      <td>97.660000</td>\n",
              "      <td>686.500000</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>0.099930</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.080040</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.813129e+06</td>\n",
              "      <td>15.780000</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>104.100000</td>\n",
              "      <td>782.700000</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>...</td>\n",
              "      <td>29.720000</td>\n",
              "      <td>125.400000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>0.382900</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.092080</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.113205e+08</td>\n",
              "      <td>28.110000</td>\n",
              "      <td>39.280000</td>\n",
              "      <td>188.500000</td>\n",
              "      <td>2501.000000</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.426800</td>\n",
              "      <td>0.201200</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>...</td>\n",
              "      <td>49.540000</td>\n",
              "      <td>251.200000</td>\n",
              "      <td>4254.000000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>1.058000</td>\n",
              "      <td>1.252000</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b11f137-4109-40c0-aa6e-c20de4216487')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9b11f137-4109-40c0-aa6e-c20de4216487 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9b11f137-4109-40c0-aa6e-c20de4216487');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-52c80fd6-3f8c-42b3-80a4-d79e0885647f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52c80fd6-3f8c-42b3-80a4-d79e0885647f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-52c80fd6-3f8c-42b3-80a4-d79e0885647f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing of the dataset\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/data.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "RN8OzkIonTL9",
        "outputId": "2e6f38d2-e0d5-4f7d-deb5-2391ea1bcb91"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0      842302         M        17.99         10.38          122.80     1001.0   \n",
              "1      842517         M        20.57         17.77          132.90     1326.0   \n",
              "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
              "3    84348301         M        11.42         20.38           77.58      386.1   \n",
              "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
              "..        ...       ...          ...           ...             ...        ...   \n",
              "564    926424         M        21.56         22.39          142.00     1479.0   \n",
              "565    926682         M        20.13         28.25          131.20     1261.0   \n",
              "566    926954         M        16.60         28.08          108.30      858.1   \n",
              "567    927241         M        20.60         29.33          140.10     1265.0   \n",
              "568     92751         B         7.76         24.54           47.92      181.0   \n",
              "\n",
              "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0            0.11840           0.27760         0.30010              0.14710   \n",
              "1            0.08474           0.07864         0.08690              0.07017   \n",
              "2            0.10960           0.15990         0.19740              0.12790   \n",
              "3            0.14250           0.28390         0.24140              0.10520   \n",
              "4            0.10030           0.13280         0.19800              0.10430   \n",
              "..               ...               ...             ...                  ...   \n",
              "564          0.11100           0.11590         0.24390              0.13890   \n",
              "565          0.09780           0.10340         0.14400              0.09791   \n",
              "566          0.08455           0.10230         0.09251              0.05302   \n",
              "567          0.11780           0.27700         0.35140              0.15200   \n",
              "568          0.05263           0.04362         0.00000              0.00000   \n",
              "\n",
              "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
              "0    ...          17.33           184.60      2019.0           0.16220   \n",
              "1    ...          23.41           158.80      1956.0           0.12380   \n",
              "2    ...          25.53           152.50      1709.0           0.14440   \n",
              "3    ...          26.50            98.87       567.7           0.20980   \n",
              "4    ...          16.67           152.20      1575.0           0.13740   \n",
              "..   ...            ...              ...         ...               ...   \n",
              "564  ...          26.40           166.10      2027.0           0.14100   \n",
              "565  ...          38.25           155.00      1731.0           0.11660   \n",
              "566  ...          34.12           126.70      1124.0           0.11390   \n",
              "567  ...          39.42           184.60      1821.0           0.16500   \n",
              "568  ...          30.37            59.16       268.6           0.08996   \n",
              "\n",
              "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
              "0              0.66560           0.7119                0.2654          0.4601   \n",
              "1              0.18660           0.2416                0.1860          0.2750   \n",
              "2              0.42450           0.4504                0.2430          0.3613   \n",
              "3              0.86630           0.6869                0.2575          0.6638   \n",
              "4              0.20500           0.4000                0.1625          0.2364   \n",
              "..                 ...              ...                   ...             ...   \n",
              "564            0.21130           0.4107                0.2216          0.2060   \n",
              "565            0.19220           0.3215                0.1628          0.2572   \n",
              "566            0.30940           0.3403                0.1418          0.2218   \n",
              "567            0.86810           0.9387                0.2650          0.4087   \n",
              "568            0.06444           0.0000                0.0000          0.2871   \n",
              "\n",
              "     fractal_dimension_worst  Unnamed: 32  \n",
              "0                    0.11890          NaN  \n",
              "1                    0.08902          NaN  \n",
              "2                    0.08758          NaN  \n",
              "3                    0.17300          NaN  \n",
              "4                    0.07678          NaN  \n",
              "..                       ...          ...  \n",
              "564                  0.07115          NaN  \n",
              "565                  0.06637          NaN  \n",
              "566                  0.07820          NaN  \n",
              "567                  0.12400          NaN  \n",
              "568                  0.07039          NaN  \n",
              "\n",
              "[569 rows x 33 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-106924ce-60e4-469f-ba4c-c5f56b082770\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>926424</td>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>...</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>926682</td>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>...</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>926954</td>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>...</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>927241</td>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>...</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>92751</td>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 33 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-106924ce-60e4-469f-ba4c-c5f56b082770')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-106924ce-60e4-469f-ba4c-c5f56b082770 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-106924ce-60e4-469f-ba4c-c5f56b082770');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1d2e3b20-beb4-4762-ab7f-6f28977a0def\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1d2e3b20-beb4-4762-ab7f-6f28977a0def')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1d2e3b20-beb4-4762-ab7f-6f28977a0def button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing of the dataset\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/data.csv')\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhAUqLfZhIR_",
        "outputId": "72be75b4-d77b-4f10-e616-52bb489c8ad5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
            "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
            "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
            "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
            "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
            "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
            "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
            "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
            "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes\n",
        "#The Listed out all are the features of the dataset used for the classification."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irh6_G9injFC",
        "outputId": "9c2594b2-0a03-4f18-a6a3-626cf6b97586"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                           int64\n",
              "diagnosis                   object\n",
              "radius_mean                float64\n",
              "texture_mean               float64\n",
              "perimeter_mean             float64\n",
              "area_mean                  float64\n",
              "smoothness_mean            float64\n",
              "compactness_mean           float64\n",
              "concavity_mean             float64\n",
              "concave points_mean        float64\n",
              "symmetry_mean              float64\n",
              "fractal_dimension_mean     float64\n",
              "radius_se                  float64\n",
              "texture_se                 float64\n",
              "perimeter_se               float64\n",
              "area_se                    float64\n",
              "smoothness_se              float64\n",
              "compactness_se             float64\n",
              "concavity_se               float64\n",
              "concave points_se          float64\n",
              "symmetry_se                float64\n",
              "fractal_dimension_se       float64\n",
              "radius_worst               float64\n",
              "texture_worst              float64\n",
              "perimeter_worst            float64\n",
              "area_worst                 float64\n",
              "smoothness_worst           float64\n",
              "compactness_worst          float64\n",
              "concavity_worst            float64\n",
              "concave points_worst       float64\n",
              "symmetry_worst             float64\n",
              "fractal_dimension_worst    float64\n",
              "Unnamed: 32                float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8VuZq2qn05k",
        "outputId": "4a9f7c86-2697-4779-a1d6-2b28cff3d11c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                           0\n",
              "diagnosis                    0\n",
              "radius_mean                  0\n",
              "texture_mean                 0\n",
              "perimeter_mean               0\n",
              "area_mean                    0\n",
              "smoothness_mean              0\n",
              "compactness_mean             0\n",
              "concavity_mean               0\n",
              "concave points_mean          0\n",
              "symmetry_mean                0\n",
              "fractal_dimension_mean       0\n",
              "radius_se                    0\n",
              "texture_se                   0\n",
              "perimeter_se                 0\n",
              "area_se                      0\n",
              "smoothness_se                0\n",
              "compactness_se               0\n",
              "concavity_se                 0\n",
              "concave points_se            0\n",
              "symmetry_se                  0\n",
              "fractal_dimension_se         0\n",
              "radius_worst                 0\n",
              "texture_worst                0\n",
              "perimeter_worst              0\n",
              "area_worst                   0\n",
              "smoothness_worst             0\n",
              "compactness_worst            0\n",
              "concavity_worst              0\n",
              "concave points_worst         0\n",
              "symmetry_worst               0\n",
              "fractal_dimension_worst      0\n",
              "Unnamed: 32                569\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ],
      "metadata": {
        "id": "Jcv97yYKkN8p"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load your dataset (replace 'your_dataset.csv' with your actual file)\n",
        "df = pd.read_csv('/content/data.csv')\n",
        "\n",
        "# Assume 'X' contains features and 'y' contains labels\n",
        "label_column_name = 'diagnosis'\n",
        "X = df.drop(label_column_name, axis=1)\n",
        "y = df[label_column_name]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Impute missing values with the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Train the model with logistic regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display results\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Classification Report:\\n', report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Divjm066ggfX",
        "outputId": "d2bb14bb-ad11-40e6-e826-498e8ff19700"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6228070175438597\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       0.62      1.00      0.77        71\n",
            "           M       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.62       114\n",
            "   macro avg       0.31      0.50      0.38       114\n",
            "weighted avg       0.39      0.62      0.48       114\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "# Replace NaN values with the mean of each column\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Train the model with imputed data\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_imputed)\n"
      ],
      "metadata": {
        "id": "mTDyhidEnf6S"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the original dataset\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Impute missing values with the mean\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Train the model with imputed data\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_imputed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jltqzkmcnoCz",
        "outputId": "3fd14cd1-22af-4c29-b66f-069e1e2ea2f8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id                           0\n",
            "diagnosis                    0\n",
            "radius_mean                  0\n",
            "texture_mean                 0\n",
            "perimeter_mean               0\n",
            "area_mean                    0\n",
            "smoothness_mean              0\n",
            "compactness_mean             0\n",
            "concavity_mean               0\n",
            "concave points_mean          0\n",
            "symmetry_mean                0\n",
            "fractal_dimension_mean       0\n",
            "radius_se                    0\n",
            "texture_se                   0\n",
            "perimeter_se                 0\n",
            "area_se                      0\n",
            "smoothness_se                0\n",
            "compactness_se               0\n",
            "concavity_se                 0\n",
            "concave points_se            0\n",
            "symmetry_se                  0\n",
            "fractal_dimension_se         0\n",
            "radius_worst                 0\n",
            "texture_worst                0\n",
            "perimeter_worst              0\n",
            "area_worst                   0\n",
            "smoothness_worst             0\n",
            "compactness_worst            0\n",
            "concavity_worst              0\n",
            "concave points_worst         0\n",
            "symmetry_worst               0\n",
            "fractal_dimension_worst      0\n",
            "Unnamed: 32                569\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#naive Bayes Classifier\n",
        "#Dispalying of the Results\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming you have your X_train_imputed, X_test_imputed, y_train, and y_test prepared\n",
        "\n",
        "# Create a Naive Bayes classifier\n",
        "naive_bayes_classifier = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "naive_bayes_classifier.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_naive_bayes = naive_bayes_classifier.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_naive_bayes = accuracy_score(y_test, y_pred_naive_bayes)\n",
        "report_naive_bayes = classification_report(y_test, y_pred_naive_bayes)\n",
        "\n",
        "# Display results\n",
        "print(f'Accuracy (Naive Bayes): {accuracy_naive_bayes}')\n",
        "print('Classification Report (Naive Bayes):\\n', report_naive_bayes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNx7FJqdmlq0",
        "outputId": "ccc4063f-2d5f-4283-e5fb-32c57c051a4e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Naive Bayes): 0.39473684210526316\n",
            "Classification Report (Naive Bayes):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       0.56      0.13      0.21        71\n",
            "           M       0.37      0.84      0.51        43\n",
            "\n",
            "    accuracy                           0.39       114\n",
            "   macro avg       0.46      0.48      0.36       114\n",
            "weighted avg       0.49      0.39      0.32       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression\n",
        "#Displaying of the Result Accuracy\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/data.csv')\n",
        "\n",
        "# Drop 'Unnamed: 32' column\n",
        "df = df.drop('Unnamed: 32', axis=1)\n",
        "\n",
        "# Assume 'diagnosis' contains labels\n",
        "label_column_name = 'diagnosis'\n",
        "X = df.drop(label_column_name, axis=1)\n",
        "y = df[label_column_name]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Impute missing values with the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Train the model with imputed data\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display results\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Classification Report:\\n', report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sP5v7FToru-",
        "outputId": "d694e51b-f6e7-4349-d37c-a996b1965fd6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6228070175438597\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       0.62      1.00      0.77        71\n",
            "           M       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.62       114\n",
            "   macro avg       0.31      0.50      0.38       114\n",
            "weighted avg       0.39      0.62      0.48       114\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_weighted = LogisticRegression(class_weight={0: 0.7, 1: 0.3})"
      ],
      "metadata": {
        "id": "sBovUiJzqazU"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imbalanced-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMVwXQHaqdxc",
        "outputId": "16416d07-befb-44fa-c4ba-98a50af1153d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression\n",
        "#Displaying of the Result Accuracy\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/data.csv')\n",
        "\n",
        "# Drop 'Unnamed: 32' column\n",
        "df = df.drop('Unnamed: 32', axis=1)\n",
        "\n",
        "# Assume 'diagnosis' contains labels\n",
        "label_column_name = 'diagnosis'\n",
        "X = df.drop(label_column_name, axis=1)\n",
        "y = df[label_column_name]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Impute missing values with the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Train the model with imputed data\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display results\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Classification Report:\\n', report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi5Sw_8UFHq9",
        "outputId": "4a054c98-406d-4622-cd4a-c703d381cd98"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6228070175438597\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       0.62      1.00      0.77        71\n",
            "           M       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.62       114\n",
            "   macro avg       0.31      0.50      0.38       114\n",
            "weighted avg       0.39      0.62      0.48       114\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/data.csv')\n",
        "\n",
        "# Drop 'Unnamed: 32' column\n",
        "df = df.drop('Unnamed: 32', axis=1)\n",
        "\n",
        "# Assume 'diagnosis' contains labels\n",
        "label_column_name = 'diagnosis'\n",
        "X = df.drop(label_column_name, axis=1)\n",
        "y = df[label_column_name]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Impute missing values with the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Train the model with class weights\n",
        "model_weighted = LogisticRegression(class_weight='balanced')\n",
        "model_weighted.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_weighted = model_weighted.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the model with class weights\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "report_weighted = classification_report(y_test, y_pred_weighted)\n",
        "\n",
        "# Display results with class weights\n",
        "print(f'Accuracy (with class weights): {accuracy_weighted}')\n",
        "print('Classification Report (with class weights):\\n', report_weighted)\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrix for the model with class weights\n",
        "plot_confusion_matrix(y_test, y_pred_weighted, 'Confusion Matrix (Class Weights)')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "PH8AS8L9irxX",
        "outputId": "a50322ff-a919-4160-9e77-ad3a11d24fcd"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (with class weights): 0.37719298245614036\n",
            "Classification Report (with class weights):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       0.00      0.00      0.00        71\n",
            "           M       0.38      1.00      0.55        43\n",
            "\n",
            "    accuracy                           0.38       114\n",
            "   macro avg       0.19      0.50      0.27       114\n",
            "weighted avg       0.14      0.38      0.21       114\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of RandomUnderSampler\n",
        "# for resampling process\n",
        "\n",
        "!pip install imbalanced-learn\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "under_sampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "# Fit and apply the undersampler to the training data\n",
        "X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train_imputed, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Gvfkq0shhCS",
        "outputId": "5ed00e5e-0e5e-4cd2-dc8e-266ac2ed8813"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/data.csv')\n",
        "\n",
        "# Drop 'Unnamed: 32' column\n",
        "df = df.drop('Unnamed: 32', axis=1)\n",
        "\n",
        "# Assume 'diagnosis' contains labels\n",
        "label_column_name = 'diagnosis'\n",
        "X = df.drop(label_column_name, axis=1)\n",
        "y = df[label_column_name]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Impute missing values with the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Create an instance of RandomUnderSampler\n",
        "under_sampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "\n",
        "# Fit and apply the undersampler to the training data\n",
        "X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train_imputed, y_train)\n",
        "\n",
        "# Train the model on the resampled data\n",
        "model_resampled = LogisticRegression()\n",
        "model_resampled.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_resampled = model_resampled.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the model with resampling\n",
        "accuracy_resampled = accuracy_score(y_test, y_pred_resampled)\n",
        "report_resampled = classification_report(y_test, y_pred_resampled)\n",
        "\n",
        "# Display results with resampling\n",
        "print(f'Accuracy (with resampling): {accuracy_resampled}')\n",
        "print('Classification Report (with resampling):\\n', report_resampled)\n",
        "\n",
        "# Plot confusion matrix for the model with resampling\n",
        "plot_confusion_matrix(y_test, y_pred_resampled, 'Confusion Matrix (Resampling)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "8Q-Cx3sZjH8z",
        "outputId": "13d084d8-da5d-4590-8dd8-a7ea8b003bfe"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (with resampling): 0.37719298245614036\n",
            "Classification Report (with resampling):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       0.00      0.00      0.00        71\n",
            "           M       0.38      1.00      0.55        43\n",
            "\n",
            "    accuracy                           0.38       114\n",
            "   macro avg       0.19      0.50      0.27       114\n",
            "weighted avg       0.14      0.38      0.21       114\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CONFUSION MATRIX\n",
        "#Displaying with the results\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrix for the models\n",
        "plot_confusion_matrix(y_test, y_pred_weighted, 'Confusion Matrix (Class Weights)')\n",
        "plot_confusion_matrix(y_test, y_pred_resampled, 'Confusion Matrix (Resampling)')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "jKG4Fur4rC7X",
        "outputId": "82029f5f-91bf-4952-e970-ca9d6efc703b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/data.csv')"
      ],
      "metadata": {
        "id": "J5H5UuN8TyAn"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Random Forest Classifier from sklearn.ensemble\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Importing metrics from sklearn library\n",
        "from sklearn import metrics\n",
        "\n",
        "# Assuming X_train and y_train are already defined\n",
        "# Create an instance of RandomForestClassifier with random_state=5\n",
        "rf_classifier = RandomForestClassifier(random_state=5)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rf_classifier.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "SdSiC6lsUquL",
        "outputId": "861ee5c9-fa3a-4473-9c4d-af48d64f3167"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=5)"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=5)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Local Variables for the running of the Random Forest Classifier\n",
        "%whos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbyozVcvVcE_",
        "outputId": "4c7f6a7c-a0ca-47c4-cf42-22fefd1e1b27"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variable                 Type                      Data/Info\n",
            "------------------------------------------------------------\n",
            "DecisionTreeClassifier   ABCMeta                   <class 'sklearn.tree._cla<...>.DecisionTreeClassifier'>\n",
            "LogisticRegression       type                      <class 'sklearn.linear_mo<...>stic.LogisticRegression'>\n",
            "MultinomialNB            ABCMeta                   <class 'sklearn.naive_bayes.MultinomialNB'>\n",
            "RandomForestClassifier   ABCMeta                   <class 'sklearn.ensemble.<...>.RandomForestClassifier'>\n",
            "RandomUnderSampler       ABCMeta                   <class 'imblearn.under_sa<...>pler.RandomUnderSampler'>\n",
            "SVC                      ABCMeta                   <class 'sklearn.svm._classes.SVC'>\n",
            "SimpleImputer            type                      <class 'sklearn.impute._base.SimpleImputer'>\n",
            "X                        DataFrame                            id  radius_mea<...>\\n[569 rows x 31 columns]\n",
            "X_test                   DataFrame                            id  radius_mea<...>\\n[114 rows x 31 columns]\n",
            "X_test_imputed           ndarray                   114x31: 3534 elems, type `float64`, 28272 bytes\n",
            "X_train                  DataFrame                            id  radius_mea<...>\\n[455 rows x 31 columns]\n",
            "X_train_imputed          ndarray                   455x31: 14105 elems, type `float64`, 112840 bytes (110.1953125 kb)\n",
            "X_train_resampled        ndarray                   338x31: 10478 elems, type `float64`, 83824 bytes\n",
            "acc                      float64                   0.30701754385964913\n",
            "accuracy                 float64                   0.6228070175438597\n",
            "accuracy_dt_classifier   float64                   0.9473684210526315\n",
            "accuracy_naive_bayes     float64                   0.39473684210526316\n",
            "accuracy_resampled       float64                   0.37719298245614036\n",
            "accuracy_rf              float64                   0.9649122807017544\n",
            "accuracy_score           function                  <function accuracy_score at 0x7bb8d4569990>\n",
            "accuracy_svm             float64                   0.6228070175438597\n",
            "accuracy_weighted        float64                   0.37719298245614036\n",
            "classification_report    function                  <function classification_<...>report at 0x7bb8d456a290>\n",
            "confusion_matrix         function                  <function confusion_matrix at 0x7bb8d4569870>\n",
            "df                       DataFrame                            id diagnosis  <...>\\n[569 rows x 33 columns]\n",
            "dt_classifier            DecisionTreeClassifier    DecisionTreeClassifier(random_state=5)\n",
            "imputer                  SimpleImputer             SimpleImputer()\n",
            "label_column_name        str                       diagnosis\n",
            "log                      LogisticRegression        LogisticRegression()\n",
            "metrics                  module                    <module 'sklearn.metrics'<...>arn/metrics/__init__.py'>\n",
            "model                    LogisticRegression        LogisticRegression()\n",
            "model_resampled          LogisticRegression        LogisticRegression()\n",
            "model_weighted           LogisticRegression        LogisticRegression(class_weight='balanced')\n",
            "naive_bayes_classifier   MultinomialNB             MultinomialNB()\n",
            "np                       module                    <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n",
            "pd                       module                    <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n",
            "plot_confusion_matrix    function                  <function plot_confusion_<...>matrix at 0x7bb8c8a8e5f0>\n",
            "plt                      module                    <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
            "report                   str                                     precision  <...>.62      0.48       114\\n\n",
            "report_dt_classifier     str                                     precision  <...>.95      0.95       114\\n\n",
            "report_naive_bayes       str                                     precision  <...>.39      0.32       114\\n\n",
            "report_resampled         str                                     precision  <...>.38      0.21       114\\n\n",
            "report_rf                str                                     precision  <...>.96      0.96       114\\n\n",
            "report_svm               str                                     precision  <...>.62      0.48       114\\n\n",
            "report_weighted          str                                     precision  <...>.38      0.21       114\\n\n",
            "rf_classifier            RandomForestClassifier    RandomForestClassifier(random_state=5)\n",
            "sns                      module                    <module 'seaborn' from '/<...>ges/seaborn/__init__.py'>\n",
            "svm_classifier           SVC                       SVC()\n",
            "train_test_split         function                  <function train_test_split at 0x7bb8d45431c0>\n",
            "under_sampler            RandomUnderSampler        RandomUnderSampler(random_state=42)\n",
            "x                        DataFrame                            id  radius_mea<...>\\n[569 rows x 31 columns]\n",
            "x_test                   DataFrame                            id  radius_mea<...>\\n[114 rows x 31 columns]\n",
            "x_train                  DataFrame                            id  radius_mea<...>\\n[455 rows x 31 columns]\n",
            "y                        Series                    0      M\\n1      M\\n2    <...>ength: 569, dtype: object\n",
            "y_pred                   ndarray                   114: 114 elems, type `object`, 912 bytes\n",
            "y_pred_dt_classifier     ndarray                   114: 114 elems, type `object`, 912 bytes\n",
            "y_pred_naive_bayes       ndarray                   114: 114 elems, type `<U1`, 456 bytes\n",
            "y_pred_resampled         ndarray                   114: 114 elems, type `object`, 912 bytes\n",
            "y_pred_rf                ndarray                   114: 114 elems, type `object`, 912 bytes\n",
            "y_pred_svm               ndarray                   114: 114 elems, type `object`, 912 bytes\n",
            "y_pred_weighted          ndarray                   114: 114 elems, type `object`, 912 bytes\n",
            "y_test                   Series                    204    B\\n70     M\\n131  <...>ength: 114, dtype: object\n",
            "y_train                  Series                    68     B\\n181    M\\n63   <...>ength: 455, dtype: object\n",
            "y_train_resampled        Series                    0      B\\n1      B\\n2    <...>ength: 338, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(locals().keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13a5UKwAVgIJ",
        "outputId": "ac14635e-bcc8-4ea8-b0e6-9a10713c2d23"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', '_', '__', '___', '_i', '_ii', '_iii', '_i1', 'SVC', 'accuracy_score', 'classification_report', 'svm_classifier', '_i2', 'DecisionTreeClassifier', 'dt_classifier', '_i3', '_i4', '_exit_code', '_i5', 'np', 'pd', 'train_test_split', 'LogisticRegression', '_i6', 'SimpleImputer', 'imputer', '_i7', 'df', '_7', '_i8', '_8', '_i9', '_9', '_i10', '_i11', '_11', '_i12', '_12', '_i13', '_i14', '_i15', 'label_column_name', 'X', 'y', 'X_train', 'X_test', 'y_train', 'y_test', 'X_train_imputed', 'X_test_imputed', 'model', 'y_pred', 'accuracy', 'report', '_i16', '_i17', '_i18', 'MultinomialNB', 'naive_bayes_classifier', 'y_pred_naive_bayes', 'accuracy_naive_bayes', 'report_naive_bayes', '_i19', '_i20', 'model_weighted', '_i21', '_i22', '_i23', 'plt', 'sns', 'confusion_matrix', 'y_pred_weighted', 'accuracy_weighted', 'report_weighted', 'plot_confusion_matrix', '_i24', 'RandomUnderSampler', 'under_sampler', 'X_train_resampled', 'y_train_resampled', '_i25', 'model_resampled', 'y_pred_resampled', 'accuracy_resampled', 'report_resampled', '_i26', '_i27', '_i28', 'RandomForestClassifier', 'metrics', 'rf_classifier', '_28', '_i29', '_i30', '_i31', '_31', '_i32', 'y_pred_rf', 'accuracy_rf', 'report_rf', '_i33', '_i34', '_i35', 'y_pred_dt_classifier', 'accuracy_dt_classifier', 'report_dt_classifier', '_i36', 'y_pred_svm', 'accuracy_svm', 'report_svm', '_i37', '_37', '_i38', '_38', '_i39', '_i40', '_i41', '_41', '_i42', '_i43', '_43', '_i44', 'x', '_i45', '_45', '_i46', '_i47', 'x_train', 'x_test', '_i48', '_i49', 'log', '_i50', '_50', '_i51', '_i52', 'acc', '_i53', '_i54', '_i55', '_i56', '_i57', '_i58', '_i59', '_i60', '_i61', '_i62', '_i63', '_63', '_i64', '_i65'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a RandomForestClassifier\n",
        "rf_classifier = RandomForestClassifier(random_state=5)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rf_classifier.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "nyo_7d11WYOv",
        "outputId": "fd1c885f-e726-4792-cc28-2c219a3aceca"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=5)"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=5)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RandomForest Classification\n",
        "#Displaying the Results Accuracy\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "report_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "# Display results\n",
        "print(f'Accuracy: {accuracy_rf}')\n",
        "print('Classification Report:\\n', report_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id0gC0tVWfff",
        "outputId": "57595c7a-51e2-4202-bda3-ab053fec208e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649122807017544\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       0.96      0.99      0.97        71\n",
            "           M       0.98      0.93      0.95        43\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.96      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjBzDbwEUloV",
        "outputId": "5d6679b4-400e-43b4-b5b0-fef2d49078aa"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[70  1]\n",
            " [ 3 40]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%whos  #it is used for printing of the variables present in the given dataset."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZYquWsRYc-n",
        "outputId": "5cc63f8a-b50c-4a86-97ca-b919a0cd836e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No variables match your requested type.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(locals().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uUsxc9nYfbs",
        "outputId": "2348d835-a769-4b4b-d0ba-5949be2a4f89"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', '_', '__', '___', '_i', '_ii', '_iii', '_i1', 'SVC', 'accuracy_score', 'classification_report', 'svm_classifier', '_i2', 'DecisionTreeClassifier', 'dt_classifier', '_i3', '_i4', '_exit_code', '_i5', 'np', 'pd', 'train_test_split', 'LogisticRegression', '_i6', 'SimpleImputer', 'imputer', '_i7', 'df', '_7', '_i8', '_8', '_i9', '_9', '_i10', '_i11', '_11', '_i12', '_12', '_i13', '_i14', '_i15', 'label_column_name', 'X', 'y', 'X_train', 'X_test', 'y_train', 'y_test', 'X_train_imputed', 'X_test_imputed', 'model', 'y_pred', 'accuracy', 'report', '_i16', '_i17', '_i18', 'MultinomialNB', 'naive_bayes_classifier', 'y_pred_naive_bayes', 'accuracy_naive_bayes', 'report_naive_bayes', '_i19', '_i20', 'model_weighted', '_i21', '_i22', '_i23', 'plt', 'sns', 'confusion_matrix', 'y_pred_weighted', 'accuracy_weighted', 'report_weighted', 'plot_confusion_matrix', '_i24', 'RandomUnderSampler', 'under_sampler', 'X_train_resampled', 'y_train_resampled', '_i25', 'model_resampled', 'y_pred_resampled', 'accuracy_resampled', 'report_resampled', '_i26', '_i27', '_i28', 'RandomForestClassifier', 'metrics', 'rf_classifier', '_28', '_i29', '_i30', '_i31', '_31', '_i32', 'y_pred_rf', 'accuracy_rf', 'report_rf', '_i33', '_i34'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree\n",
        "#Displaying of the Result Accuracy\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Create and train the DecisionTreeClassifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=5)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_dt_classifier = dt_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy_dt_classifier = accuracy_score(y_test, y_pred_dt_classifier)\n",
        "report_dt_classifier = classification_report(y_test, y_pred_dt_classifier)\n",
        "\n",
        "print(f'Accuracy (Decision Tree Classifier): {accuracy_dt_classifier}')\n",
        "print('Classification Report (Decision Tree Classifier):\\n', report_dt_classifier)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5gxiyizl4jw",
        "outputId": "20dda4b3-e050-4da2-d5f0-bb67f6b18db3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Decision Tree Classifier): 0.9473684210526315\n",
            "Classification Report (Decision Tree Classifier):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       0.96      0.96      0.96        71\n",
            "           M       0.93      0.93      0.93        43\n",
            "\n",
            "    accuracy                           0.95       114\n",
            "   macro avg       0.94      0.94      0.94       114\n",
            "weighted avg       0.95      0.95      0.95       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test,y_pred_dt_classifier)\n",
        "cm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4pRvnQPXbbE",
        "outputId": "ca6735ff-5584-42ae-f469-fe0feb46ab1c"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[68,  3],\n",
              "       [ 3, 40]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM Classifier\n",
        "#Dispalying the accuracy Result\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming you have your X_train_imputed, X_test_imputed, y_train, and y_test prepared\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm_classifier = SVC()\n",
        "\n",
        "# Train the model\n",
        "svm_classifier.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_svm = svm_classifier.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "report_svm = classification_report(y_test, y_pred_svm)\n",
        "\n",
        "# Display results\n",
        "print(f'Accuracy (SVM): {accuracy_svm}')\n",
        "print('Classification Report (SVM):\\n', report_svm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F54Mcs-qm5SX",
        "outputId": "42b32a60-0862-41bd-c936-f60fee42e151"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (SVM): 0.6228070175438597\n",
            "Classification Report (SVM):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       0.62      1.00      0.77        71\n",
            "           M       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.62       114\n",
            "   macro avg       0.31      0.50      0.38       114\n",
            "weighted avg       0.39      0.62      0.48       114\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming you have your X_train_imputed, X_test_imputed, y_train, and y_test prepared\n",
        "\n",
        "# Encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Reshape the input data to match the CNN format (assuming one-dimensional data)\n",
        "input_shape = (X_train_imputed.shape[1], 1)\n",
        "X_train_reshaped = X_train_imputed.reshape(-1, *input_shape)\n",
        "X_test_reshaped = X_test_imputed.reshape(-1, *input_shape)\n",
        "\n",
        "# Build the CNN model for one-dimensional data\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv1D(32, 3, activation='relu', input_shape=input_shape))\n",
        "model.add(layers.MaxPooling1D(2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_reshaped, y_train_encoded, epochs=1000, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_cnn = model.evaluate(X_test_reshaped, y_test_encoded)[1]\n",
        "\n",
        "# Display result\n",
        "print(f'Accuracy (CNN): {accuracy_cnn}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz2U8f-ub-Dy",
        "outputId": "e989b8e2-c7ec-4ecb-a504-83d41d94795f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "12/12 [==============================] - 3s 71ms/step - loss: 205088.7031 - accuracy: 0.5027 - val_loss: 23429.2852 - val_accuracy: 0.3626\n",
            "Epoch 2/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 134612.5000 - accuracy: 0.5714 - val_loss: 8135.8442 - val_accuracy: 0.3626\n",
            "Epoch 3/1000\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 18752.7500 - accuracy: 0.4698 - val_loss: 55645.7969 - val_accuracy: 0.6703\n",
            "Epoch 4/1000\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 136088.4219 - accuracy: 0.5549 - val_loss: 17811.1172 - val_accuracy: 0.3626\n",
            "Epoch 5/1000\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 58493.5898 - accuracy: 0.5797 - val_loss: 7821.2734 - val_accuracy: 0.3626\n",
            "Epoch 6/1000\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 73441.7422 - accuracy: 0.5357 - val_loss: 162657.9062 - val_accuracy: 0.6703\n",
            "Epoch 7/1000\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 75819.3828 - accuracy: 0.5082 - val_loss: 22249.4395 - val_accuracy: 0.6923\n",
            "Epoch 8/1000\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 24035.0879 - accuracy: 0.5137 - val_loss: 1152.0939 - val_accuracy: 0.3626\n",
            "Epoch 9/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 56354.1211 - accuracy: 0.4863 - val_loss: 31802.9746 - val_accuracy: 0.6374\n",
            "Epoch 10/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 152106.0156 - accuracy: 0.6071 - val_loss: 8407.9492 - val_accuracy: 0.3626\n",
            "Epoch 11/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 35845.9023 - accuracy: 0.5247 - val_loss: 23427.6270 - val_accuracy: 0.3626\n",
            "Epoch 12/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 68123.9766 - accuracy: 0.5137 - val_loss: 891.7227 - val_accuracy: 0.3626\n",
            "Epoch 13/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 21102.8906 - accuracy: 0.5440 - val_loss: 40065.9688 - val_accuracy: 0.6374\n",
            "Epoch 14/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 70577.9922 - accuracy: 0.5110 - val_loss: 8392.1006 - val_accuracy: 0.3626\n",
            "Epoch 15/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 13080.5469 - accuracy: 0.5577 - val_loss: 24801.8789 - val_accuracy: 0.6923\n",
            "Epoch 16/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 39020.3828 - accuracy: 0.5220 - val_loss: 16580.1094 - val_accuracy: 0.3626\n",
            "Epoch 17/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 64631.7148 - accuracy: 0.5137 - val_loss: 21054.4980 - val_accuracy: 0.3626\n",
            "Epoch 18/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 57671.4570 - accuracy: 0.4835 - val_loss: 21714.3965 - val_accuracy: 0.7253\n",
            "Epoch 19/1000\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 63303.0781 - accuracy: 0.4560 - val_loss: 26291.9551 - val_accuracy: 0.3626\n",
            "Epoch 20/1000\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 99637.9141 - accuracy: 0.5247 - val_loss: 118961.8359 - val_accuracy: 0.6813\n",
            "Epoch 21/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 116469.0781 - accuracy: 0.4725 - val_loss: 28374.4668 - val_accuracy: 0.3626\n",
            "Epoch 22/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 183469.4688 - accuracy: 0.6099 - val_loss: 189448.3906 - val_accuracy: 0.6703\n",
            "Epoch 23/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 118341.4922 - accuracy: 0.5357 - val_loss: 28369.5215 - val_accuracy: 0.3626\n",
            "Epoch 24/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 82063.1484 - accuracy: 0.5632 - val_loss: 8244.3828 - val_accuracy: 0.3626\n",
            "Epoch 25/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 33065.5039 - accuracy: 0.4835 - val_loss: 6906.4136 - val_accuracy: 0.4725\n",
            "Epoch 26/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9509.0059 - accuracy: 0.5110 - val_loss: 7991.8062 - val_accuracy: 0.3626\n",
            "Epoch 27/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 13806.3271 - accuracy: 0.5412 - val_loss: 12906.8926 - val_accuracy: 0.8022\n",
            "Epoch 28/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 18918.1836 - accuracy: 0.5082 - val_loss: 3431.7710 - val_accuracy: 0.3626\n",
            "Epoch 29/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 22172.2734 - accuracy: 0.5330 - val_loss: 15597.5342 - val_accuracy: 0.6923\n",
            "Epoch 30/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 49118.4023 - accuracy: 0.4698 - val_loss: 52500.8828 - val_accuracy: 0.6813\n",
            "Epoch 31/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 59241.9492 - accuracy: 0.4560 - val_loss: 112964.9219 - val_accuracy: 0.6703\n",
            "Epoch 32/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 95215.6016 - accuracy: 0.6209 - val_loss: 15170.6699 - val_accuracy: 0.3626\n",
            "Epoch 33/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 64076.1641 - accuracy: 0.5192 - val_loss: 1191.0797 - val_accuracy: 0.3626\n",
            "Epoch 34/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 21397.7578 - accuracy: 0.5495 - val_loss: 26138.7578 - val_accuracy: 0.6813\n",
            "Epoch 35/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 35665.3086 - accuracy: 0.5769 - val_loss: 13339.1455 - val_accuracy: 0.3626\n",
            "Epoch 36/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 92886.6406 - accuracy: 0.4533 - val_loss: 26962.5254 - val_accuracy: 0.6813\n",
            "Epoch 37/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 32934.6797 - accuracy: 0.4890 - val_loss: 34416.9297 - val_accuracy: 0.6374\n",
            "Epoch 38/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 22201.6777 - accuracy: 0.5357 - val_loss: 42020.0469 - val_accuracy: 0.6484\n",
            "Epoch 39/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 76733.3672 - accuracy: 0.5192 - val_loss: 34071.2930 - val_accuracy: 0.6374\n",
            "Epoch 40/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 63606.9102 - accuracy: 0.4973 - val_loss: 2044.1862 - val_accuracy: 0.3626\n",
            "Epoch 41/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 43604.1211 - accuracy: 0.6016 - val_loss: 12048.0303 - val_accuracy: 0.3626\n",
            "Epoch 42/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 39978.3555 - accuracy: 0.4093 - val_loss: 26484.7285 - val_accuracy: 0.6813\n",
            "Epoch 43/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 68050.5156 - accuracy: 0.5330 - val_loss: 21251.7559 - val_accuracy: 0.3626\n",
            "Epoch 44/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 68380.8984 - accuracy: 0.4725 - val_loss: 15933.9248 - val_accuracy: 0.3626\n",
            "Epoch 45/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 85282.3828 - accuracy: 0.4615 - val_loss: 208960.7500 - val_accuracy: 0.6593\n",
            "Epoch 46/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 267086.3125 - accuracy: 0.6236 - val_loss: 61510.6172 - val_accuracy: 0.6374\n",
            "Epoch 47/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 48357.2695 - accuracy: 0.4478 - val_loss: 43527.4570 - val_accuracy: 0.6593\n",
            "Epoch 48/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 91141.3125 - accuracy: 0.4560 - val_loss: 2673.5107 - val_accuracy: 0.4945\n",
            "Epoch 49/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 30337.0273 - accuracy: 0.5440 - val_loss: 12051.2393 - val_accuracy: 0.3626\n",
            "Epoch 50/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 18453.2207 - accuracy: 0.5275 - val_loss: 2410.5205 - val_accuracy: 0.4835\n",
            "Epoch 51/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 36022.5234 - accuracy: 0.5357 - val_loss: 61877.2734 - val_accuracy: 0.6813\n",
            "Epoch 52/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 28486.5996 - accuracy: 0.4918 - val_loss: 68090.8984 - val_accuracy: 0.6813\n",
            "Epoch 53/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 65887.0078 - accuracy: 0.5137 - val_loss: 30930.9805 - val_accuracy: 0.6703\n",
            "Epoch 54/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 45874.5938 - accuracy: 0.5357 - val_loss: 4459.9434 - val_accuracy: 0.3626\n",
            "Epoch 55/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7738.1265 - accuracy: 0.5357 - val_loss: 9323.3926 - val_accuracy: 0.7912\n",
            "Epoch 56/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7609.9346 - accuracy: 0.4753 - val_loss: 4465.5562 - val_accuracy: 0.6813\n",
            "Epoch 57/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14004.9414 - accuracy: 0.5330 - val_loss: 1136.9939 - val_accuracy: 0.5824\n",
            "Epoch 58/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7733.5864 - accuracy: 0.5632 - val_loss: 9419.8193 - val_accuracy: 0.7143\n",
            "Epoch 59/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 79673.6953 - accuracy: 0.6099 - val_loss: 25317.2051 - val_accuracy: 0.3626\n",
            "Epoch 60/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 102375.8672 - accuracy: 0.3681 - val_loss: 81366.0000 - val_accuracy: 0.6593\n",
            "Epoch 61/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 137380.3125 - accuracy: 0.6154 - val_loss: 281.8857 - val_accuracy: 0.3626\n",
            "Epoch 62/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 28603.6367 - accuracy: 0.5797 - val_loss: 4318.6001 - val_accuracy: 0.5824\n",
            "Epoch 63/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 52967.8477 - accuracy: 0.5604 - val_loss: 18212.5156 - val_accuracy: 0.3626\n",
            "Epoch 64/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 31252.7012 - accuracy: 0.5632 - val_loss: 7497.6543 - val_accuracy: 0.3626\n",
            "Epoch 65/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 41764.9961 - accuracy: 0.5220 - val_loss: 17135.9043 - val_accuracy: 0.3626\n",
            "Epoch 66/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 83818.7500 - accuracy: 0.4478 - val_loss: 63015.8750 - val_accuracy: 0.6813\n",
            "Epoch 67/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 79715.6094 - accuracy: 0.4615 - val_loss: 12975.0684 - val_accuracy: 0.3626\n",
            "Epoch 68/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 83331.7969 - accuracy: 0.5522 - val_loss: 9942.5146 - val_accuracy: 0.3626\n",
            "Epoch 69/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 66014.2891 - accuracy: 0.4808 - val_loss: 3107.4453 - val_accuracy: 0.3626\n",
            "Epoch 70/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 26911.2305 - accuracy: 0.4835 - val_loss: 4682.5454 - val_accuracy: 0.7802\n",
            "Epoch 71/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 22221.7598 - accuracy: 0.6209 - val_loss: 17038.6133 - val_accuracy: 0.7143\n",
            "Epoch 72/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 27297.7031 - accuracy: 0.4231 - val_loss: 33970.7539 - val_accuracy: 0.6813\n",
            "Epoch 73/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 54435.7930 - accuracy: 0.5330 - val_loss: 32901.2734 - val_accuracy: 0.3626\n",
            "Epoch 74/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 145647.4531 - accuracy: 0.4643 - val_loss: 94565.4375 - val_accuracy: 0.6703\n",
            "Epoch 75/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 66091.5547 - accuracy: 0.4643 - val_loss: 4802.7134 - val_accuracy: 0.3626\n",
            "Epoch 76/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 54744.8477 - accuracy: 0.6181 - val_loss: 10824.1797 - val_accuracy: 0.3626\n",
            "Epoch 77/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 98242.0547 - accuracy: 0.3956 - val_loss: 45162.8477 - val_accuracy: 0.6813\n",
            "Epoch 78/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 167488.6094 - accuracy: 0.6346 - val_loss: 74479.2188 - val_accuracy: 0.6813\n",
            "Epoch 79/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 119622.1328 - accuracy: 0.4148 - val_loss: 4284.8618 - val_accuracy: 0.3626\n",
            "Epoch 80/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 110409.6016 - accuracy: 0.6126 - val_loss: 128612.8594 - val_accuracy: 0.6703\n",
            "Epoch 81/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 100560.0078 - accuracy: 0.4753 - val_loss: 23532.1035 - val_accuracy: 0.3626\n",
            "Epoch 82/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 83392.5234 - accuracy: 0.5852 - val_loss: 23231.0957 - val_accuracy: 0.7253\n",
            "Epoch 83/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7989.1870 - accuracy: 0.5330 - val_loss: 18081.2227 - val_accuracy: 0.7582\n",
            "Epoch 84/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 72561.8594 - accuracy: 0.6374 - val_loss: 12128.1104 - val_accuracy: 0.3626\n",
            "Epoch 85/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 38810.5430 - accuracy: 0.4258 - val_loss: 45382.2812 - val_accuracy: 0.6703\n",
            "Epoch 86/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 71941.4219 - accuracy: 0.6154 - val_loss: 15687.3350 - val_accuracy: 0.3626\n",
            "Epoch 87/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 63697.6523 - accuracy: 0.4451 - val_loss: 21300.8301 - val_accuracy: 0.7363\n",
            "Epoch 88/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 19118.0117 - accuracy: 0.5275 - val_loss: 2165.7900 - val_accuracy: 0.5055\n",
            "Epoch 89/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8621.3369 - accuracy: 0.5962 - val_loss: 4763.3721 - val_accuracy: 0.3626\n",
            "Epoch 90/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 17188.6055 - accuracy: 0.5247 - val_loss: 6747.1978 - val_accuracy: 0.3626\n",
            "Epoch 91/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 17179.5742 - accuracy: 0.5659 - val_loss: 31330.9121 - val_accuracy: 0.6703\n",
            "Epoch 92/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 115685.1328 - accuracy: 0.6401 - val_loss: 48590.5312 - val_accuracy: 0.6813\n",
            "Epoch 93/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 53913.9766 - accuracy: 0.4588 - val_loss: 24238.4531 - val_accuracy: 0.7143\n",
            "Epoch 94/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 17373.5254 - accuracy: 0.5962 - val_loss: 5594.9561 - val_accuracy: 0.3626\n",
            "Epoch 95/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14579.7305 - accuracy: 0.5687 - val_loss: 165.5362 - val_accuracy: 0.4396\n",
            "Epoch 96/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11681.5664 - accuracy: 0.6264 - val_loss: 14493.5664 - val_accuracy: 0.7363\n",
            "Epoch 97/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4795.6284 - accuracy: 0.6319 - val_loss: 23306.5000 - val_accuracy: 0.7033\n",
            "Epoch 98/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 16560.4219 - accuracy: 0.6566 - val_loss: 24783.7305 - val_accuracy: 0.7143\n",
            "Epoch 99/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 16645.7656 - accuracy: 0.6209 - val_loss: 7667.3096 - val_accuracy: 0.8132\n",
            "Epoch 100/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 30682.6074 - accuracy: 0.5549 - val_loss: 17902.3242 - val_accuracy: 0.3626\n",
            "Epoch 101/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 73583.2500 - accuracy: 0.5412 - val_loss: 26373.9883 - val_accuracy: 0.7253\n",
            "Epoch 102/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 15806.5000 - accuracy: 0.5330 - val_loss: 62116.2148 - val_accuracy: 0.6813\n",
            "Epoch 103/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 63439.5156 - accuracy: 0.5522 - val_loss: 2329.3413 - val_accuracy: 0.3626\n",
            "Epoch 104/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 116314.2344 - accuracy: 0.6016 - val_loss: 40966.1172 - val_accuracy: 0.6813\n",
            "Epoch 105/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 27896.7578 - accuracy: 0.4670 - val_loss: 26351.4531 - val_accuracy: 0.7363\n",
            "Epoch 106/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 39434.1211 - accuracy: 0.6209 - val_loss: 12331.8252 - val_accuracy: 0.3626\n",
            "Epoch 107/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 23226.0957 - accuracy: 0.5247 - val_loss: 918.9311 - val_accuracy: 0.4835\n",
            "Epoch 108/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8891.6816 - accuracy: 0.6154 - val_loss: 2182.5947 - val_accuracy: 0.3626\n",
            "Epoch 109/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10648.5449 - accuracy: 0.6346 - val_loss: 1935.7394 - val_accuracy: 0.3626\n",
            "Epoch 110/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 19807.9512 - accuracy: 0.4341 - val_loss: 100601.8203 - val_accuracy: 0.6703\n",
            "Epoch 111/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 137933.9062 - accuracy: 0.6401 - val_loss: 45693.9492 - val_accuracy: 0.6813\n",
            "Epoch 112/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 73651.2734 - accuracy: 0.3956 - val_loss: 9717.4473 - val_accuracy: 0.3626\n",
            "Epoch 113/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 31173.9805 - accuracy: 0.6044 - val_loss: 6253.1484 - val_accuracy: 0.3626\n",
            "Epoch 114/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 15270.0635 - accuracy: 0.6319 - val_loss: 3846.4636 - val_accuracy: 0.3736\n",
            "Epoch 115/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 62791.3398 - accuracy: 0.5220 - val_loss: 21659.1699 - val_accuracy: 0.3626\n",
            "Epoch 116/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 83548.9375 - accuracy: 0.5522 - val_loss: 11869.8086 - val_accuracy: 0.3626\n",
            "Epoch 117/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 34072.8203 - accuracy: 0.4945 - val_loss: 72840.6172 - val_accuracy: 0.6703\n",
            "Epoch 118/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 71855.7578 - accuracy: 0.5632 - val_loss: 38387.9219 - val_accuracy: 0.3626\n",
            "Epoch 119/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 74291.1797 - accuracy: 0.4176 - val_loss: 72696.1953 - val_accuracy: 0.6703\n",
            "Epoch 120/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 90187.4297 - accuracy: 0.6456 - val_loss: 3982.2356 - val_accuracy: 0.6923\n",
            "Epoch 121/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 37579.0117 - accuracy: 0.5220 - val_loss: 30913.0820 - val_accuracy: 0.7253\n",
            "Epoch 122/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 12674.2168 - accuracy: 0.5137 - val_loss: 7021.5410 - val_accuracy: 0.3626\n",
            "Epoch 123/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 30740.6270 - accuracy: 0.6923 - val_loss: 5404.2012 - val_accuracy: 0.3736\n",
            "Epoch 124/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 36204.9414 - accuracy: 0.4835 - val_loss: 36141.0000 - val_accuracy: 0.7033\n",
            "Epoch 125/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 26102.3359 - accuracy: 0.5577 - val_loss: 33723.0664 - val_accuracy: 0.7253\n",
            "Epoch 126/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 50601.9180 - accuracy: 0.5659 - val_loss: 17080.3633 - val_accuracy: 0.7692\n",
            "Epoch 127/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 21205.5859 - accuracy: 0.6319 - val_loss: 1445.0479 - val_accuracy: 0.3736\n",
            "Epoch 128/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 7193.0366 - accuracy: 0.6621 - val_loss: 5586.1841 - val_accuracy: 0.3736\n",
            "Epoch 129/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 17369.6094 - accuracy: 0.6456 - val_loss: 3246.5276 - val_accuracy: 0.3956\n",
            "Epoch 130/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 34162.3086 - accuracy: 0.5055 - val_loss: 33748.8828 - val_accuracy: 0.7033\n",
            "Epoch 131/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 23387.4004 - accuracy: 0.5989 - val_loss: 17765.3672 - val_accuracy: 0.7363\n",
            "Epoch 132/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 9848.3740 - accuracy: 0.6621 - val_loss: 41683.8750 - val_accuracy: 0.6703\n",
            "Epoch 133/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 19572.9395 - accuracy: 0.6126 - val_loss: 8113.0166 - val_accuracy: 0.8352\n",
            "Epoch 134/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 35769.4609 - accuracy: 0.6346 - val_loss: 12721.4043 - val_accuracy: 0.3626\n",
            "Epoch 135/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 32715.8906 - accuracy: 0.5687 - val_loss: 7649.0947 - val_accuracy: 0.3736\n",
            "Epoch 136/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7591.6431 - accuracy: 0.5275 - val_loss: 6981.9478 - val_accuracy: 0.4066\n",
            "Epoch 137/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 19745.7109 - accuracy: 0.5714 - val_loss: 6493.6572 - val_accuracy: 0.3956\n",
            "Epoch 138/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8451.5020 - accuracy: 0.7390 - val_loss: 4774.1587 - val_accuracy: 0.7473\n",
            "Epoch 139/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8441.9951 - accuracy: 0.6703 - val_loss: 24780.4922 - val_accuracy: 0.6923\n",
            "Epoch 140/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16488.9922 - accuracy: 0.6841 - val_loss: 7832.2910 - val_accuracy: 0.4066\n",
            "Epoch 141/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 26071.4395 - accuracy: 0.5714 - val_loss: 1152.0022 - val_accuracy: 0.8462\n",
            "Epoch 142/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 28386.9473 - accuracy: 0.6841 - val_loss: 15678.2363 - val_accuracy: 0.7363\n",
            "Epoch 143/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 20782.2480 - accuracy: 0.6264 - val_loss: 1135.1914 - val_accuracy: 0.5385\n",
            "Epoch 144/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14660.3896 - accuracy: 0.5852 - val_loss: 2330.1274 - val_accuracy: 0.4066\n",
            "Epoch 145/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10284.8350 - accuracy: 0.6786 - val_loss: 11944.6816 - val_accuracy: 0.4066\n",
            "Epoch 146/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 35576.7695 - accuracy: 0.5412 - val_loss: 45453.5625 - val_accuracy: 0.6593\n",
            "Epoch 147/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 35930.4492 - accuracy: 0.6099 - val_loss: 14966.9619 - val_accuracy: 0.3956\n",
            "Epoch 148/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 40755.2305 - accuracy: 0.5797 - val_loss: 16263.6914 - val_accuracy: 0.7473\n",
            "Epoch 149/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 18612.2852 - accuracy: 0.4698 - val_loss: 30536.4707 - val_accuracy: 0.7143\n",
            "Epoch 150/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 58336.4180 - accuracy: 0.6621 - val_loss: 2904.5151 - val_accuracy: 0.4066\n",
            "Epoch 151/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 90902.0234 - accuracy: 0.3819 - val_loss: 533.4915 - val_accuracy: 0.5495\n",
            "Epoch 152/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 169741.1719 - accuracy: 0.6511 - val_loss: 172331.8125 - val_accuracy: 0.6703\n",
            "Epoch 153/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 61253.1719 - accuracy: 0.6538 - val_loss: 23145.1406 - val_accuracy: 0.3626\n",
            "Epoch 154/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 116362.2109 - accuracy: 0.3736 - val_loss: 14401.8984 - val_accuracy: 0.7912\n",
            "Epoch 155/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 69144.3672 - accuracy: 0.7060 - val_loss: 2979.2905 - val_accuracy: 0.3626\n",
            "Epoch 156/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 41169.1797 - accuracy: 0.5165 - val_loss: 44976.4883 - val_accuracy: 0.7253\n",
            "Epoch 157/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 32134.2988 - accuracy: 0.5852 - val_loss: 4410.9224 - val_accuracy: 0.6593\n",
            "Epoch 158/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 12705.9531 - accuracy: 0.6291 - val_loss: 2814.2874 - val_accuracy: 0.6923\n",
            "Epoch 159/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4556.2593 - accuracy: 0.5632 - val_loss: 31859.0820 - val_accuracy: 0.7253\n",
            "Epoch 160/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 28179.8652 - accuracy: 0.6758 - val_loss: 15562.2422 - val_accuracy: 0.3736\n",
            "Epoch 161/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 50918.6523 - accuracy: 0.5687 - val_loss: 9141.2207 - val_accuracy: 0.8352\n",
            "Epoch 162/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 45957.3398 - accuracy: 0.5357 - val_loss: 33856.0586 - val_accuracy: 0.7143\n",
            "Epoch 163/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 18029.7383 - accuracy: 0.6593 - val_loss: 7059.3120 - val_accuracy: 0.4066\n",
            "Epoch 164/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10836.3857 - accuracy: 0.5495 - val_loss: 6782.8228 - val_accuracy: 0.8242\n",
            "Epoch 165/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 13510.7910 - accuracy: 0.7143 - val_loss: 1665.4669 - val_accuracy: 0.8462\n",
            "Epoch 166/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 15745.0098 - accuracy: 0.6648 - val_loss: 5499.6821 - val_accuracy: 0.3956\n",
            "Epoch 167/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16260.3477 - accuracy: 0.5412 - val_loss: 22439.5762 - val_accuracy: 0.7143\n",
            "Epoch 168/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 13403.1172 - accuracy: 0.7527 - val_loss: 4589.8735 - val_accuracy: 0.3956\n",
            "Epoch 169/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 15865.4570 - accuracy: 0.6236 - val_loss: 3767.3076 - val_accuracy: 0.3956\n",
            "Epoch 170/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5233.4023 - accuracy: 0.7253 - val_loss: 2256.6697 - val_accuracy: 0.8242\n",
            "Epoch 171/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 14538.8047 - accuracy: 0.5879 - val_loss: 28286.5137 - val_accuracy: 0.6923\n",
            "Epoch 172/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 35510.9297 - accuracy: 0.6291 - val_loss: 9170.7549 - val_accuracy: 0.7473\n",
            "Epoch 173/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 41719.0898 - accuracy: 0.6951 - val_loss: 1096.4709 - val_accuracy: 0.6154\n",
            "Epoch 174/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 25264.3574 - accuracy: 0.7088 - val_loss: 10089.3076 - val_accuracy: 0.4066\n",
            "Epoch 175/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 24997.2227 - accuracy: 0.5330 - val_loss: 31835.5469 - val_accuracy: 0.7143\n",
            "Epoch 176/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16974.2832 - accuracy: 0.6621 - val_loss: 8525.7930 - val_accuracy: 0.8352\n",
            "Epoch 177/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 18423.7520 - accuracy: 0.6703 - val_loss: 20308.1914 - val_accuracy: 0.7473\n",
            "Epoch 178/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16189.5400 - accuracy: 0.6923 - val_loss: 10585.3291 - val_accuracy: 0.8022\n",
            "Epoch 179/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6726.9458 - accuracy: 0.6758 - val_loss: 10148.1885 - val_accuracy: 0.8022\n",
            "Epoch 180/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11888.2598 - accuracy: 0.6786 - val_loss: 4689.7720 - val_accuracy: 0.8242\n",
            "Epoch 181/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 15178.8682 - accuracy: 0.6648 - val_loss: 25020.5000 - val_accuracy: 0.7143\n",
            "Epoch 182/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 21704.3301 - accuracy: 0.7610 - val_loss: 11119.5166 - val_accuracy: 0.7692\n",
            "Epoch 183/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 15873.3320 - accuracy: 0.6951 - val_loss: 15424.7578 - val_accuracy: 0.7582\n",
            "Epoch 184/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8642.8018 - accuracy: 0.6731 - val_loss: 34485.1328 - val_accuracy: 0.7143\n",
            "Epoch 185/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 42932.5391 - accuracy: 0.6429 - val_loss: 21899.5449 - val_accuracy: 0.7473\n",
            "Epoch 186/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 36088.0195 - accuracy: 0.7280 - val_loss: 44552.1719 - val_accuracy: 0.7143\n",
            "Epoch 187/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 18230.3750 - accuracy: 0.5659 - val_loss: 16649.5879 - val_accuracy: 0.8022\n",
            "Epoch 188/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 12974.6211 - accuracy: 0.7143 - val_loss: 26161.1211 - val_accuracy: 0.7473\n",
            "Epoch 189/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 27513.2715 - accuracy: 0.7280 - val_loss: 40277.5508 - val_accuracy: 0.7143\n",
            "Epoch 190/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9286.5850 - accuracy: 0.7060 - val_loss: 4553.1162 - val_accuracy: 0.7692\n",
            "Epoch 191/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9405.1836 - accuracy: 0.5879 - val_loss: 10381.9639 - val_accuracy: 0.8352\n",
            "Epoch 192/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10400.8945 - accuracy: 0.7390 - val_loss: 10557.7178 - val_accuracy: 0.8242\n",
            "Epoch 193/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 19123.1758 - accuracy: 0.6319 - val_loss: 19422.3770 - val_accuracy: 0.7473\n",
            "Epoch 194/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 23586.2363 - accuracy: 0.7692 - val_loss: 6944.4834 - val_accuracy: 0.8242\n",
            "Epoch 195/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7887.2886 - accuracy: 0.6401 - val_loss: 3398.0913 - val_accuracy: 0.4505\n",
            "Epoch 196/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 20515.5391 - accuracy: 0.6978 - val_loss: 1506.7504 - val_accuracy: 0.6923\n",
            "Epoch 197/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 19940.7305 - accuracy: 0.7555 - val_loss: 8283.5225 - val_accuracy: 0.3956\n",
            "Epoch 198/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 22921.8789 - accuracy: 0.6346 - val_loss: 17169.9434 - val_accuracy: 0.7473\n",
            "Epoch 199/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 39018.9102 - accuracy: 0.6648 - val_loss: 11198.1025 - val_accuracy: 0.4066\n",
            "Epoch 200/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 26127.6094 - accuracy: 0.5852 - val_loss: 22350.1230 - val_accuracy: 0.7253\n",
            "Epoch 201/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 30325.5391 - accuracy: 0.5604 - val_loss: 29730.9414 - val_accuracy: 0.7143\n",
            "Epoch 202/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 63468.4766 - accuracy: 0.6813 - val_loss: 4682.6860 - val_accuracy: 0.8132\n",
            "Epoch 203/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 13574.5195 - accuracy: 0.6703 - val_loss: 28940.3770 - val_accuracy: 0.7473\n",
            "Epoch 204/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12151.8848 - accuracy: 0.6566 - val_loss: 2803.1980 - val_accuracy: 0.7582\n",
            "Epoch 205/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3691.1570 - accuracy: 0.7170 - val_loss: 4649.8604 - val_accuracy: 0.8462\n",
            "Epoch 206/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5790.2246 - accuracy: 0.7253 - val_loss: 7420.8091 - val_accuracy: 0.4066\n",
            "Epoch 207/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 18742.2344 - accuracy: 0.6786 - val_loss: 35388.5391 - val_accuracy: 0.7143\n",
            "Epoch 208/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 18161.9844 - accuracy: 0.7060 - val_loss: 28634.4395 - val_accuracy: 0.7473\n",
            "Epoch 209/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 25103.1914 - accuracy: 0.7005 - val_loss: 11355.9170 - val_accuracy: 0.8352\n",
            "Epoch 210/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3240.8320 - accuracy: 0.7335 - val_loss: 1154.1096 - val_accuracy: 0.5165\n",
            "Epoch 211/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7639.9272 - accuracy: 0.7527 - val_loss: 21892.6875 - val_accuracy: 0.7582\n",
            "Epoch 212/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 13028.1191 - accuracy: 0.6621 - val_loss: 2273.0771 - val_accuracy: 0.8352\n",
            "Epoch 213/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 17254.1270 - accuracy: 0.4808 - val_loss: 4225.5708 - val_accuracy: 0.4066\n",
            "Epoch 214/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 24802.8379 - accuracy: 0.7198 - val_loss: 10366.0312 - val_accuracy: 0.3956\n",
            "Epoch 215/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 25051.8379 - accuracy: 0.5357 - val_loss: 70729.7500 - val_accuracy: 0.6593\n",
            "Epoch 216/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 89876.1172 - accuracy: 0.6676 - val_loss: 40.2426 - val_accuracy: 0.8681\n",
            "Epoch 217/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 43287.0312 - accuracy: 0.4945 - val_loss: 12679.8643 - val_accuracy: 0.7802\n",
            "Epoch 218/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 16400.0117 - accuracy: 0.7198 - val_loss: 2013.1610 - val_accuracy: 0.8462\n",
            "Epoch 219/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5032.6470 - accuracy: 0.7940 - val_loss: 1992.1055 - val_accuracy: 0.5934\n",
            "Epoch 220/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 14102.1289 - accuracy: 0.6511 - val_loss: 4480.8203 - val_accuracy: 0.4396\n",
            "Epoch 221/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 31903.9277 - accuracy: 0.6154 - val_loss: 20813.2539 - val_accuracy: 0.7253\n",
            "Epoch 222/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7922.3408 - accuracy: 0.7610 - val_loss: 10387.1230 - val_accuracy: 0.8022\n",
            "Epoch 223/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11393.2764 - accuracy: 0.7885 - val_loss: 2579.0291 - val_accuracy: 0.8571\n",
            "Epoch 224/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10069.1904 - accuracy: 0.6841 - val_loss: 6543.1167 - val_accuracy: 0.8242\n",
            "Epoch 225/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6693.9175 - accuracy: 0.8324 - val_loss: 8289.6182 - val_accuracy: 0.8242\n",
            "Epoch 226/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10725.9707 - accuracy: 0.7445 - val_loss: 21178.7695 - val_accuracy: 0.7473\n",
            "Epoch 227/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 43881.6914 - accuracy: 0.5934 - val_loss: 22352.0254 - val_accuracy: 0.7473\n",
            "Epoch 228/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 39224.5820 - accuracy: 0.6374 - val_loss: 37876.3008 - val_accuracy: 0.7143\n",
            "Epoch 229/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 15458.9551 - accuracy: 0.7363 - val_loss: 4857.0420 - val_accuracy: 0.4066\n",
            "Epoch 230/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 25994.1328 - accuracy: 0.7033 - val_loss: 1253.7177 - val_accuracy: 0.6923\n",
            "Epoch 231/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8239.3838 - accuracy: 0.7995 - val_loss: 2196.3184 - val_accuracy: 0.5934\n",
            "Epoch 232/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 17114.1094 - accuracy: 0.7473 - val_loss: 6632.7734 - val_accuracy: 0.3956\n",
            "Epoch 233/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 21948.0215 - accuracy: 0.6264 - val_loss: 6965.8345 - val_accuracy: 0.8242\n",
            "Epoch 234/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6628.7007 - accuracy: 0.7692 - val_loss: 4327.5068 - val_accuracy: 0.5165\n",
            "Epoch 235/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 15952.6143 - accuracy: 0.6566 - val_loss: 1602.9093 - val_accuracy: 0.7253\n",
            "Epoch 236/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 13249.0234 - accuracy: 0.7445 - val_loss: 13435.4160 - val_accuracy: 0.7582\n",
            "Epoch 237/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 15719.8857 - accuracy: 0.6566 - val_loss: 16501.9746 - val_accuracy: 0.7473\n",
            "Epoch 238/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 30652.1406 - accuracy: 0.7500 - val_loss: 11296.3174 - val_accuracy: 0.3956\n",
            "Epoch 239/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 41125.3125 - accuracy: 0.5467 - val_loss: 14908.6523 - val_accuracy: 0.7582\n",
            "Epoch 240/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 26899.3926 - accuracy: 0.6538 - val_loss: 23863.2910 - val_accuracy: 0.7143\n",
            "Epoch 241/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 26817.7695 - accuracy: 0.7060 - val_loss: 4740.0386 - val_accuracy: 0.5165\n",
            "Epoch 242/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5760.3892 - accuracy: 0.7940 - val_loss: 1766.1000 - val_accuracy: 0.7033\n",
            "Epoch 243/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8930.8809 - accuracy: 0.8132 - val_loss: 9827.7617 - val_accuracy: 0.7912\n",
            "Epoch 244/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 12968.4775 - accuracy: 0.7527 - val_loss: 4671.1167 - val_accuracy: 0.8462\n",
            "Epoch 245/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11129.5020 - accuracy: 0.8269 - val_loss: 9230.5293 - val_accuracy: 0.8242\n",
            "Epoch 246/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9711.0342 - accuracy: 0.7720 - val_loss: 3163.1506 - val_accuracy: 0.5385\n",
            "Epoch 247/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 15819.6240 - accuracy: 0.7665 - val_loss: 6315.4175 - val_accuracy: 0.3956\n",
            "Epoch 248/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 29625.0664 - accuracy: 0.5852 - val_loss: 17102.7051 - val_accuracy: 0.7692\n",
            "Epoch 249/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6851.0488 - accuracy: 0.7830 - val_loss: 5000.0459 - val_accuracy: 0.4505\n",
            "Epoch 250/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 46856.2031 - accuracy: 0.4670 - val_loss: 10185.6377 - val_accuracy: 0.7582\n",
            "Epoch 251/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12162.8350 - accuracy: 0.7747 - val_loss: 25521.1289 - val_accuracy: 0.7033\n",
            "Epoch 252/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 32963.2773 - accuracy: 0.6016 - val_loss: 2068.1289 - val_accuracy: 0.7253\n",
            "Epoch 253/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8743.8721 - accuracy: 0.7802 - val_loss: 628.0937 - val_accuracy: 0.7253\n",
            "Epoch 254/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 5426.1162 - accuracy: 0.8242 - val_loss: 12754.0664 - val_accuracy: 0.7692\n",
            "Epoch 255/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 15846.7871 - accuracy: 0.7143 - val_loss: 17714.6738 - val_accuracy: 0.7473\n",
            "Epoch 256/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 14983.3984 - accuracy: 0.7610 - val_loss: 7300.5840 - val_accuracy: 0.8242\n",
            "Epoch 257/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 4351.2329 - accuracy: 0.8242 - val_loss: 2049.2480 - val_accuracy: 0.5934\n",
            "Epoch 258/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 7411.1860 - accuracy: 0.7335 - val_loss: 3259.4182 - val_accuracy: 0.5055\n",
            "Epoch 259/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 33260.5234 - accuracy: 0.7335 - val_loss: 924.5040 - val_accuracy: 0.6264\n",
            "Epoch 260/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 20769.7598 - accuracy: 0.6401 - val_loss: 33457.6992 - val_accuracy: 0.7473\n",
            "Epoch 261/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 28116.5586 - accuracy: 0.6401 - val_loss: 12934.7051 - val_accuracy: 0.4066\n",
            "Epoch 262/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 56254.1445 - accuracy: 0.5742 - val_loss: 65681.3672 - val_accuracy: 0.7143\n",
            "Epoch 263/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 38678.1055 - accuracy: 0.7143 - val_loss: 14152.6553 - val_accuracy: 0.4066\n",
            "Epoch 264/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 44391.2070 - accuracy: 0.6236 - val_loss: 11758.9189 - val_accuracy: 0.8242\n",
            "Epoch 265/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 18604.9082 - accuracy: 0.5687 - val_loss: 22339.4023 - val_accuracy: 0.7473\n",
            "Epoch 266/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 22131.9902 - accuracy: 0.7610 - val_loss: 4857.3262 - val_accuracy: 0.4176\n",
            "Epoch 267/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 29811.2637 - accuracy: 0.5907 - val_loss: 30921.8027 - val_accuracy: 0.7473\n",
            "Epoch 268/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 26955.2910 - accuracy: 0.7115 - val_loss: 18064.5488 - val_accuracy: 0.7582\n",
            "Epoch 269/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 21164.6387 - accuracy: 0.7005 - val_loss: 8129.7090 - val_accuracy: 0.3956\n",
            "Epoch 270/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 18474.5605 - accuracy: 0.7115 - val_loss: 166.8211 - val_accuracy: 0.8681\n",
            "Epoch 271/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 32968.1172 - accuracy: 0.7280 - val_loss: 1572.0951 - val_accuracy: 0.7033\n",
            "Epoch 272/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 14766.2695 - accuracy: 0.7033 - val_loss: 5416.8203 - val_accuracy: 0.8242\n",
            "Epoch 273/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 16536.2148 - accuracy: 0.7802 - val_loss: 11417.6318 - val_accuracy: 0.8242\n",
            "Epoch 274/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 9808.3828 - accuracy: 0.7473 - val_loss: 1242.4904 - val_accuracy: 0.8462\n",
            "Epoch 275/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 17621.7852 - accuracy: 0.7692 - val_loss: 2027.3784 - val_accuracy: 0.5934\n",
            "Epoch 276/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6732.6353 - accuracy: 0.7418 - val_loss: 6038.3198 - val_accuracy: 0.3956\n",
            "Epoch 277/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10071.6396 - accuracy: 0.7363 - val_loss: 1035.6581 - val_accuracy: 0.6923\n",
            "Epoch 278/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2381.6260 - accuracy: 0.8214 - val_loss: 6442.1230 - val_accuracy: 0.8242\n",
            "Epoch 279/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9305.9639 - accuracy: 0.7995 - val_loss: 13991.6816 - val_accuracy: 0.8022\n",
            "Epoch 280/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 26989.2832 - accuracy: 0.5824 - val_loss: 13312.5869 - val_accuracy: 0.3956\n",
            "Epoch 281/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 21386.6699 - accuracy: 0.6621 - val_loss: 4454.3838 - val_accuracy: 0.7692\n",
            "Epoch 282/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8725.9043 - accuracy: 0.8104 - val_loss: 610.3623 - val_accuracy: 0.7253\n",
            "Epoch 283/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 19747.9883 - accuracy: 0.7418 - val_loss: 5640.0640 - val_accuracy: 0.8132\n",
            "Epoch 284/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 21835.0508 - accuracy: 0.7390 - val_loss: 8552.1982 - val_accuracy: 0.3956\n",
            "Epoch 285/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 41502.2031 - accuracy: 0.6319 - val_loss: 10261.1025 - val_accuracy: 0.8022\n",
            "Epoch 286/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12787.6689 - accuracy: 0.7335 - val_loss: 9060.3545 - val_accuracy: 0.8242\n",
            "Epoch 287/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7884.9209 - accuracy: 0.7665 - val_loss: 12691.4639 - val_accuracy: 0.8242\n",
            "Epoch 288/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 13078.1719 - accuracy: 0.7885 - val_loss: 9643.5234 - val_accuracy: 0.8352\n",
            "Epoch 289/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1885.3107 - accuracy: 0.7280 - val_loss: 15528.2334 - val_accuracy: 0.8242\n",
            "Epoch 290/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 23198.0215 - accuracy: 0.6978 - val_loss: 2687.1460 - val_accuracy: 0.4945\n",
            "Epoch 291/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6626.0288 - accuracy: 0.7060 - val_loss: 13145.5205 - val_accuracy: 0.8242\n",
            "Epoch 292/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 25644.2285 - accuracy: 0.7143 - val_loss: 26416.7148 - val_accuracy: 0.7473\n",
            "Epoch 293/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 21103.9141 - accuracy: 0.7308 - val_loss: 10718.9268 - val_accuracy: 0.8242\n",
            "Epoch 294/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 27872.9316 - accuracy: 0.7335 - val_loss: 10281.1543 - val_accuracy: 0.3956\n",
            "Epoch 295/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 57913.8125 - accuracy: 0.6703 - val_loss: 23112.1016 - val_accuracy: 0.7692\n",
            "Epoch 296/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 28219.5605 - accuracy: 0.5852 - val_loss: 14303.9971 - val_accuracy: 0.8242\n",
            "Epoch 297/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16742.4277 - accuracy: 0.7885 - val_loss: 2076.8257 - val_accuracy: 0.6154\n",
            "Epoch 298/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5832.5063 - accuracy: 0.7995 - val_loss: 841.5435 - val_accuracy: 0.8571\n",
            "Epoch 299/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5365.7451 - accuracy: 0.8104 - val_loss: 803.2103 - val_accuracy: 0.6923\n",
            "Epoch 300/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14173.2529 - accuracy: 0.7967 - val_loss: 21980.5312 - val_accuracy: 0.8022\n",
            "Epoch 301/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8072.0850 - accuracy: 0.7308 - val_loss: 968.1875 - val_accuracy: 0.6484\n",
            "Epoch 302/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7525.5859 - accuracy: 0.7857 - val_loss: 3295.3455 - val_accuracy: 0.8352\n",
            "Epoch 303/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3253.0049 - accuracy: 0.7885 - val_loss: 3398.2769 - val_accuracy: 0.5934\n",
            "Epoch 304/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9179.7744 - accuracy: 0.7692 - val_loss: 935.1148 - val_accuracy: 0.7033\n",
            "Epoch 305/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5062.6621 - accuracy: 0.7692 - val_loss: 1367.5782 - val_accuracy: 0.8791\n",
            "Epoch 306/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5068.2666 - accuracy: 0.7802 - val_loss: 9853.7764 - val_accuracy: 0.8242\n",
            "Epoch 307/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 15097.7061 - accuracy: 0.7418 - val_loss: 16152.6680 - val_accuracy: 0.7912\n",
            "Epoch 308/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10382.0615 - accuracy: 0.7582 - val_loss: 12502.5166 - val_accuracy: 0.8242\n",
            "Epoch 309/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 13667.3945 - accuracy: 0.7555 - val_loss: 6111.4497 - val_accuracy: 0.8462\n",
            "Epoch 310/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 21303.3848 - accuracy: 0.7857 - val_loss: 747.8725 - val_accuracy: 0.6264\n",
            "Epoch 311/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4163.3804 - accuracy: 0.7665 - val_loss: 2275.4070 - val_accuracy: 0.8352\n",
            "Epoch 312/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 12891.8525 - accuracy: 0.7665 - val_loss: 45329.0430 - val_accuracy: 0.7473\n",
            "Epoch 313/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 49049.0430 - accuracy: 0.6813 - val_loss: 796.2681 - val_accuracy: 0.6154\n",
            "Epoch 314/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5272.6416 - accuracy: 0.7940 - val_loss: 289.6538 - val_accuracy: 0.7473\n",
            "Epoch 315/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1356.7880 - accuracy: 0.8077 - val_loss: 1551.8298 - val_accuracy: 0.8571\n",
            "Epoch 316/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12654.1084 - accuracy: 0.8187 - val_loss: 2942.3921 - val_accuracy: 0.5934\n",
            "Epoch 317/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6184.1484 - accuracy: 0.6951 - val_loss: 4968.7070 - val_accuracy: 0.8462\n",
            "Epoch 318/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8768.1611 - accuracy: 0.7692 - val_loss: 23942.7090 - val_accuracy: 0.7473\n",
            "Epoch 319/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16984.5625 - accuracy: 0.7830 - val_loss: 7696.2856 - val_accuracy: 0.8352\n",
            "Epoch 320/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1696.7478 - accuracy: 0.8159 - val_loss: 553.4424 - val_accuracy: 0.6593\n",
            "Epoch 321/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3351.7979 - accuracy: 0.7912 - val_loss: 2001.2417 - val_accuracy: 0.6154\n",
            "Epoch 322/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 17557.0391 - accuracy: 0.6841 - val_loss: 23572.7500 - val_accuracy: 0.7582\n",
            "Epoch 323/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 20395.5664 - accuracy: 0.7418 - val_loss: 1819.3901 - val_accuracy: 0.6374\n",
            "Epoch 324/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11790.7607 - accuracy: 0.8077 - val_loss: 2475.9280 - val_accuracy: 0.6044\n",
            "Epoch 325/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 12059.9219 - accuracy: 0.7198 - val_loss: 10357.0889 - val_accuracy: 0.3956\n",
            "Epoch 326/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 39196.6328 - accuracy: 0.4973 - val_loss: 8543.5293 - val_accuracy: 0.7802\n",
            "Epoch 327/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 17357.7988 - accuracy: 0.7830 - val_loss: 11448.6416 - val_accuracy: 0.7473\n",
            "Epoch 328/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 11168.9385 - accuracy: 0.7885 - val_loss: 5863.8882 - val_accuracy: 0.7692\n",
            "Epoch 329/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 14434.9316 - accuracy: 0.7967 - val_loss: 3178.7798 - val_accuracy: 0.7033\n",
            "Epoch 330/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5090.4932 - accuracy: 0.8159 - val_loss: 7652.1553 - val_accuracy: 0.8242\n",
            "Epoch 331/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5692.0396 - accuracy: 0.8049 - val_loss: 236.1552 - val_accuracy: 0.8352\n",
            "Epoch 332/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 26895.9141 - accuracy: 0.7445 - val_loss: 1654.1917 - val_accuracy: 0.6484\n",
            "Epoch 333/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3230.1433 - accuracy: 0.8159 - val_loss: 7333.4902 - val_accuracy: 0.8571\n",
            "Epoch 334/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 16043.9941 - accuracy: 0.7802 - val_loss: 890.3900 - val_accuracy: 0.8462\n",
            "Epoch 335/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14241.6592 - accuracy: 0.6841 - val_loss: 1430.2737 - val_accuracy: 0.6593\n",
            "Epoch 336/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 15025.2471 - accuracy: 0.7363 - val_loss: 16815.8730 - val_accuracy: 0.8242\n",
            "Epoch 337/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7023.7334 - accuracy: 0.7500 - val_loss: 10993.0918 - val_accuracy: 0.3956\n",
            "Epoch 338/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 24408.1973 - accuracy: 0.6484 - val_loss: 27307.9707 - val_accuracy: 0.7473\n",
            "Epoch 339/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 20899.9785 - accuracy: 0.7390 - val_loss: 1698.7032 - val_accuracy: 0.8462\n",
            "Epoch 340/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 13218.7002 - accuracy: 0.7308 - val_loss: 6936.9102 - val_accuracy: 0.4176\n",
            "Epoch 341/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 17831.3125 - accuracy: 0.7280 - val_loss: 25047.4316 - val_accuracy: 0.7582\n",
            "Epoch 342/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 33903.8398 - accuracy: 0.6071 - val_loss: 2605.3132 - val_accuracy: 0.6593\n",
            "Epoch 343/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11563.7021 - accuracy: 0.7555 - val_loss: 10476.4854 - val_accuracy: 0.8242\n",
            "Epoch 344/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 52403.2070 - accuracy: 0.7720 - val_loss: 15645.2119 - val_accuracy: 0.8242\n",
            "Epoch 345/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 14341.0850 - accuracy: 0.7582 - val_loss: 2861.2974 - val_accuracy: 0.8571\n",
            "Epoch 346/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 14432.4365 - accuracy: 0.6374 - val_loss: 14793.8916 - val_accuracy: 0.8242\n",
            "Epoch 347/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 20303.1055 - accuracy: 0.8077 - val_loss: 2300.5771 - val_accuracy: 0.6154\n",
            "Epoch 348/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4412.4868 - accuracy: 0.7720 - val_loss: 165.6807 - val_accuracy: 0.8132\n",
            "Epoch 349/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1919.5182 - accuracy: 0.8077 - val_loss: 4892.7319 - val_accuracy: 0.5055\n",
            "Epoch 350/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 56604.3398 - accuracy: 0.4725 - val_loss: 9217.8467 - val_accuracy: 0.8242\n",
            "Epoch 351/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16332.0176 - accuracy: 0.7885 - val_loss: 191.2914 - val_accuracy: 0.8681\n",
            "Epoch 352/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1290.8374 - accuracy: 0.8489 - val_loss: 242.0456 - val_accuracy: 0.8681\n",
            "Epoch 353/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 18339.6641 - accuracy: 0.7473 - val_loss: 2298.6201 - val_accuracy: 0.8462\n",
            "Epoch 354/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 12711.3018 - accuracy: 0.7912 - val_loss: 1515.9760 - val_accuracy: 0.7033\n",
            "Epoch 355/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3137.3193 - accuracy: 0.8214 - val_loss: 3468.6526 - val_accuracy: 0.6374\n",
            "Epoch 356/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 24694.5195 - accuracy: 0.7473 - val_loss: 6889.4971 - val_accuracy: 0.8462\n",
            "Epoch 357/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2760.1235 - accuracy: 0.8269 - val_loss: 898.7388 - val_accuracy: 0.8462\n",
            "Epoch 358/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5289.7910 - accuracy: 0.8022 - val_loss: 1454.1017 - val_accuracy: 0.6703\n",
            "Epoch 359/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1996.8916 - accuracy: 0.8104 - val_loss: 3991.4099 - val_accuracy: 0.8571\n",
            "Epoch 360/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6352.0259 - accuracy: 0.8159 - val_loss: 6934.8359 - val_accuracy: 0.8462\n",
            "Epoch 361/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5418.9229 - accuracy: 0.8104 - val_loss: 2190.0957 - val_accuracy: 0.8571\n",
            "Epoch 362/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3888.9072 - accuracy: 0.8104 - val_loss: 10619.5273 - val_accuracy: 0.8242\n",
            "Epoch 363/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 18878.0781 - accuracy: 0.7418 - val_loss: 74.4686 - val_accuracy: 0.9011\n",
            "Epoch 364/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4277.7612 - accuracy: 0.8297 - val_loss: 3950.3730 - val_accuracy: 0.6374\n",
            "Epoch 365/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 19408.0391 - accuracy: 0.7940 - val_loss: 189.1997 - val_accuracy: 0.8462\n",
            "Epoch 366/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7278.9590 - accuracy: 0.8077 - val_loss: 3801.1785 - val_accuracy: 0.5934\n",
            "Epoch 367/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11141.7920 - accuracy: 0.7473 - val_loss: 738.7107 - val_accuracy: 0.7033\n",
            "Epoch 368/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 13355.8037 - accuracy: 0.7775 - val_loss: 22213.8906 - val_accuracy: 0.8242\n",
            "Epoch 369/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 21485.4590 - accuracy: 0.7198 - val_loss: 8940.3281 - val_accuracy: 0.8352\n",
            "Epoch 370/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12311.0879 - accuracy: 0.7253 - val_loss: 19624.9609 - val_accuracy: 0.8242\n",
            "Epoch 371/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 26937.3457 - accuracy: 0.7967 - val_loss: 7472.2690 - val_accuracy: 0.4066\n",
            "Epoch 372/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 24961.9375 - accuracy: 0.6484 - val_loss: 19331.8008 - val_accuracy: 0.8242\n",
            "Epoch 373/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 15000.3750 - accuracy: 0.8022 - val_loss: 10324.3955 - val_accuracy: 0.8242\n",
            "Epoch 374/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8624.6602 - accuracy: 0.7912 - val_loss: 3390.3950 - val_accuracy: 0.8462\n",
            "Epoch 375/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9605.4297 - accuracy: 0.7637 - val_loss: 4161.4014 - val_accuracy: 0.8242\n",
            "Epoch 376/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8274.4268 - accuracy: 0.7692 - val_loss: 337.1520 - val_accuracy: 0.8352\n",
            "Epoch 377/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16274.6885 - accuracy: 0.7967 - val_loss: 2462.8823 - val_accuracy: 0.6264\n",
            "Epoch 378/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 18623.1660 - accuracy: 0.6538 - val_loss: 20957.4688 - val_accuracy: 0.8242\n",
            "Epoch 379/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 24019.3828 - accuracy: 0.7335 - val_loss: 2031.6185 - val_accuracy: 0.6484\n",
            "Epoch 380/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3770.5715 - accuracy: 0.7857 - val_loss: 943.7814 - val_accuracy: 0.6923\n",
            "Epoch 381/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8746.7588 - accuracy: 0.7418 - val_loss: 22303.7695 - val_accuracy: 0.7692\n",
            "Epoch 382/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 24501.7363 - accuracy: 0.7637 - val_loss: 4791.9053 - val_accuracy: 0.5824\n",
            "Epoch 383/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16008.9248 - accuracy: 0.7170 - val_loss: 6134.2114 - val_accuracy: 0.8462\n",
            "Epoch 384/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8587.2393 - accuracy: 0.7995 - val_loss: 7650.1475 - val_accuracy: 0.8462\n",
            "Epoch 385/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11202.6982 - accuracy: 0.7747 - val_loss: 20029.3203 - val_accuracy: 0.8022\n",
            "Epoch 386/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 17838.4785 - accuracy: 0.7198 - val_loss: 1195.1394 - val_accuracy: 0.7033\n",
            "Epoch 387/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5001.7388 - accuracy: 0.8159 - val_loss: 1879.8563 - val_accuracy: 0.8681\n",
            "Epoch 388/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2690.4956 - accuracy: 0.8269 - val_loss: 3395.8337 - val_accuracy: 0.8571\n",
            "Epoch 389/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10221.0449 - accuracy: 0.7692 - val_loss: 12365.6455 - val_accuracy: 0.8242\n",
            "Epoch 390/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 17956.3145 - accuracy: 0.7637 - val_loss: 4384.3291 - val_accuracy: 0.8462\n",
            "Epoch 391/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9509.9316 - accuracy: 0.7720 - val_loss: 4101.3247 - val_accuracy: 0.6044\n",
            "Epoch 392/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 14761.6924 - accuracy: 0.7802 - val_loss: 1139.3920 - val_accuracy: 0.7033\n",
            "Epoch 393/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 27856.4531 - accuracy: 0.6099 - val_loss: 17021.4473 - val_accuracy: 0.8242\n",
            "Epoch 394/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 18777.4336 - accuracy: 0.7692 - val_loss: 5753.6738 - val_accuracy: 0.8462\n",
            "Epoch 395/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 28700.7090 - accuracy: 0.8104 - val_loss: 573.6550 - val_accuracy: 0.8462\n",
            "Epoch 396/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 17855.4434 - accuracy: 0.5742 - val_loss: 79.8428 - val_accuracy: 0.8681\n",
            "Epoch 397/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 16864.0430 - accuracy: 0.7582 - val_loss: 127.4566 - val_accuracy: 0.8681\n",
            "Epoch 398/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 7575.4380 - accuracy: 0.8104 - val_loss: 11947.8682 - val_accuracy: 0.8242\n",
            "Epoch 399/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 9347.5176 - accuracy: 0.7637 - val_loss: 7610.1069 - val_accuracy: 0.5165\n",
            "Epoch 400/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 16760.3789 - accuracy: 0.7060 - val_loss: 26812.4180 - val_accuracy: 0.7582\n",
            "Epoch 401/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 17506.3223 - accuracy: 0.7885 - val_loss: 6582.4561 - val_accuracy: 0.5165\n",
            "Epoch 402/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 22033.1504 - accuracy: 0.7363 - val_loss: 31345.3652 - val_accuracy: 0.7692\n",
            "Epoch 403/1000\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 22003.2012 - accuracy: 0.7473 - val_loss: 3046.1877 - val_accuracy: 0.6154\n",
            "Epoch 404/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 17711.5625 - accuracy: 0.7857 - val_loss: 9674.5938 - val_accuracy: 0.8462\n",
            "Epoch 405/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3196.4832 - accuracy: 0.8297 - val_loss: 6048.3242 - val_accuracy: 0.4615\n",
            "Epoch 406/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 18887.3418 - accuracy: 0.6648 - val_loss: 13110.8184 - val_accuracy: 0.8242\n",
            "Epoch 407/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 15560.1006 - accuracy: 0.7445 - val_loss: 1562.6410 - val_accuracy: 0.8681\n",
            "Epoch 408/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3380.5051 - accuracy: 0.8571 - val_loss: 4605.7568 - val_accuracy: 0.8462\n",
            "Epoch 409/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 9747.8740 - accuracy: 0.8077 - val_loss: 1218.9556 - val_accuracy: 0.8791\n",
            "Epoch 410/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2763.7202 - accuracy: 0.8297 - val_loss: 2359.8765 - val_accuracy: 0.6813\n",
            "Epoch 411/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 17928.6484 - accuracy: 0.7912 - val_loss: 2285.0530 - val_accuracy: 0.8462\n",
            "Epoch 412/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2640.2373 - accuracy: 0.8297 - val_loss: 1399.4443 - val_accuracy: 0.8571\n",
            "Epoch 413/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 12679.1172 - accuracy: 0.7363 - val_loss: 16345.1895 - val_accuracy: 0.8242\n",
            "Epoch 414/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 27228.7891 - accuracy: 0.8132 - val_loss: 574.1200 - val_accuracy: 0.7582\n",
            "Epoch 415/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 8579.9922 - accuracy: 0.7390 - val_loss: 17769.2148 - val_accuracy: 0.8242\n",
            "Epoch 416/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 18016.1328 - accuracy: 0.7857 - val_loss: 12269.4082 - val_accuracy: 0.8242\n",
            "Epoch 417/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12874.4316 - accuracy: 0.7143 - val_loss: 4344.5562 - val_accuracy: 0.5824\n",
            "Epoch 418/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 18157.4512 - accuracy: 0.7885 - val_loss: 15396.3711 - val_accuracy: 0.8242\n",
            "Epoch 419/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 15921.9414 - accuracy: 0.7198 - val_loss: 9919.2588 - val_accuracy: 0.8242\n",
            "Epoch 420/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 59496.1758 - accuracy: 0.7857 - val_loss: 53902.8086 - val_accuracy: 0.7473\n",
            "Epoch 421/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 26732.6680 - accuracy: 0.7720 - val_loss: 2648.7263 - val_accuracy: 0.5495\n",
            "Epoch 422/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5009.4248 - accuracy: 0.7637 - val_loss: 205.0332 - val_accuracy: 0.7473\n",
            "Epoch 423/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4573.5942 - accuracy: 0.7363 - val_loss: 2123.9722 - val_accuracy: 0.6374\n",
            "Epoch 424/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3837.5935 - accuracy: 0.7857 - val_loss: 2714.0332 - val_accuracy: 0.6593\n",
            "Epoch 425/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3892.1033 - accuracy: 0.7830 - val_loss: 17476.0469 - val_accuracy: 0.8132\n",
            "Epoch 426/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 20787.9902 - accuracy: 0.7115 - val_loss: 5894.1758 - val_accuracy: 0.5824\n",
            "Epoch 427/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 27419.2852 - accuracy: 0.7857 - val_loss: 22937.7637 - val_accuracy: 0.8022\n",
            "Epoch 428/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 15943.4482 - accuracy: 0.7225 - val_loss: 6123.6904 - val_accuracy: 0.5165\n",
            "Epoch 429/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 37906.6875 - accuracy: 0.7253 - val_loss: 28972.6914 - val_accuracy: 0.7912\n",
            "Epoch 430/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5673.1025 - accuracy: 0.7967 - val_loss: 1291.9498 - val_accuracy: 0.6923\n",
            "Epoch 431/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4651.1753 - accuracy: 0.7830 - val_loss: 1529.1173 - val_accuracy: 0.8462\n",
            "Epoch 432/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6221.5742 - accuracy: 0.8187 - val_loss: 4813.4092 - val_accuracy: 0.8571\n",
            "Epoch 433/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 12046.8857 - accuracy: 0.7830 - val_loss: 1596.4274 - val_accuracy: 0.8571\n",
            "Epoch 434/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7310.6743 - accuracy: 0.8049 - val_loss: 2022.9354 - val_accuracy: 0.8571\n",
            "Epoch 435/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7624.2251 - accuracy: 0.8187 - val_loss: 9349.7373 - val_accuracy: 0.8462\n",
            "Epoch 436/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4841.6538 - accuracy: 0.8242 - val_loss: 5005.5596 - val_accuracy: 0.8352\n",
            "Epoch 437/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5552.1963 - accuracy: 0.7940 - val_loss: 1455.1304 - val_accuracy: 0.6374\n",
            "Epoch 438/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 11061.8164 - accuracy: 0.7775 - val_loss: 1015.6609 - val_accuracy: 0.7033\n",
            "Epoch 439/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9200.4941 - accuracy: 0.7830 - val_loss: 3713.6326 - val_accuracy: 0.6374\n",
            "Epoch 440/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11413.5029 - accuracy: 0.8187 - val_loss: 3178.0657 - val_accuracy: 0.6923\n",
            "Epoch 441/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 11334.6377 - accuracy: 0.7720 - val_loss: 7310.3975 - val_accuracy: 0.8462\n",
            "Epoch 442/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3072.0864 - accuracy: 0.8214 - val_loss: 1310.7258 - val_accuracy: 0.7033\n",
            "Epoch 443/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8086.0527 - accuracy: 0.8132 - val_loss: 6069.0283 - val_accuracy: 0.8462\n",
            "Epoch 444/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12801.3672 - accuracy: 0.7940 - val_loss: 920.2620 - val_accuracy: 0.8791\n",
            "Epoch 445/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10458.5596 - accuracy: 0.8049 - val_loss: 9380.0127 - val_accuracy: 0.4066\n",
            "Epoch 446/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 46168.0547 - accuracy: 0.5934 - val_loss: 25999.1641 - val_accuracy: 0.7582\n",
            "Epoch 447/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 25548.5449 - accuracy: 0.8077 - val_loss: 3246.4883 - val_accuracy: 0.6593\n",
            "Epoch 448/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 22396.5137 - accuracy: 0.7775 - val_loss: 2768.7034 - val_accuracy: 0.8462\n",
            "Epoch 449/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 14375.0879 - accuracy: 0.6538 - val_loss: 9374.3506 - val_accuracy: 0.8571\n",
            "Epoch 450/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 34495.0703 - accuracy: 0.8159 - val_loss: 4973.7769 - val_accuracy: 0.8352\n",
            "Epoch 451/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 13168.9316 - accuracy: 0.7335 - val_loss: 8657.5498 - val_accuracy: 0.8571\n",
            "Epoch 452/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7934.6504 - accuracy: 0.7857 - val_loss: 832.7675 - val_accuracy: 0.7253\n",
            "Epoch 453/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8193.5781 - accuracy: 0.7857 - val_loss: 2773.6218 - val_accuracy: 0.8571\n",
            "Epoch 454/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 36473.9766 - accuracy: 0.5687 - val_loss: 1012.6755 - val_accuracy: 0.7363\n",
            "Epoch 455/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10519.0332 - accuracy: 0.8297 - val_loss: 1934.6660 - val_accuracy: 0.8571\n",
            "Epoch 456/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5916.4761 - accuracy: 0.8132 - val_loss: 2591.7754 - val_accuracy: 0.6813\n",
            "Epoch 457/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5941.2554 - accuracy: 0.8077 - val_loss: 689.9955 - val_accuracy: 0.8681\n",
            "Epoch 458/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6545.3398 - accuracy: 0.8077 - val_loss: 1406.1287 - val_accuracy: 0.7033\n",
            "Epoch 459/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4897.3628 - accuracy: 0.7775 - val_loss: 1973.9550 - val_accuracy: 0.6923\n",
            "Epoch 460/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2318.5952 - accuracy: 0.8544 - val_loss: 3089.1428 - val_accuracy: 0.8462\n",
            "Epoch 461/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 13569.7002 - accuracy: 0.8242 - val_loss: 3400.7747 - val_accuracy: 0.6374\n",
            "Epoch 462/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 20567.3574 - accuracy: 0.7115 - val_loss: 3412.7898 - val_accuracy: 0.8462\n",
            "Epoch 463/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3750.2932 - accuracy: 0.8187 - val_loss: 1200.0201 - val_accuracy: 0.7033\n",
            "Epoch 464/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 14337.6279 - accuracy: 0.7692 - val_loss: 13862.3955 - val_accuracy: 0.8242\n",
            "Epoch 465/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8427.9336 - accuracy: 0.8049 - val_loss: 606.5112 - val_accuracy: 0.8022\n",
            "Epoch 466/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3508.9045 - accuracy: 0.8407 - val_loss: 3206.8354 - val_accuracy: 0.6923\n",
            "Epoch 467/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 12489.1348 - accuracy: 0.7802 - val_loss: 19867.3359 - val_accuracy: 0.8132\n",
            "Epoch 468/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 21289.6758 - accuracy: 0.7280 - val_loss: 1130.7689 - val_accuracy: 0.6923\n",
            "Epoch 469/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4924.6807 - accuracy: 0.8049 - val_loss: 2200.2776 - val_accuracy: 0.8681\n",
            "Epoch 470/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3923.7930 - accuracy: 0.8462 - val_loss: 5876.5098 - val_accuracy: 0.8571\n",
            "Epoch 471/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5847.5605 - accuracy: 0.7912 - val_loss: 8133.1895 - val_accuracy: 0.8571\n",
            "Epoch 472/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5197.6289 - accuracy: 0.7967 - val_loss: 2467.5137 - val_accuracy: 0.8571\n",
            "Epoch 473/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4775.1558 - accuracy: 0.7692 - val_loss: 5733.9043 - val_accuracy: 0.8352\n",
            "Epoch 474/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6794.5850 - accuracy: 0.8297 - val_loss: 1846.3636 - val_accuracy: 0.8462\n",
            "Epoch 475/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 25951.2500 - accuracy: 0.5852 - val_loss: 675.3701 - val_accuracy: 0.7473\n",
            "Epoch 476/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16268.0234 - accuracy: 0.7912 - val_loss: 2996.6260 - val_accuracy: 0.6703\n",
            "Epoch 477/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11116.6660 - accuracy: 0.7308 - val_loss: 9978.6240 - val_accuracy: 0.8242\n",
            "Epoch 478/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10951.8633 - accuracy: 0.7885 - val_loss: 8615.5889 - val_accuracy: 0.8352\n",
            "Epoch 479/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 12554.3105 - accuracy: 0.8159 - val_loss: 5063.1221 - val_accuracy: 0.5934\n",
            "Epoch 480/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 37015.2930 - accuracy: 0.5110 - val_loss: 1300.5212 - val_accuracy: 0.8571\n",
            "Epoch 481/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 27261.1094 - accuracy: 0.8132 - val_loss: 281.1954 - val_accuracy: 0.8791\n",
            "Epoch 482/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3317.7712 - accuracy: 0.8489 - val_loss: 112.7357 - val_accuracy: 0.8681\n",
            "Epoch 483/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 885.1902 - accuracy: 0.8791 - val_loss: 745.6413 - val_accuracy: 0.8571\n",
            "Epoch 484/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3494.1343 - accuracy: 0.7967 - val_loss: 10863.8389 - val_accuracy: 0.8242\n",
            "Epoch 485/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8111.7397 - accuracy: 0.8379 - val_loss: 3479.5569 - val_accuracy: 0.6264\n",
            "Epoch 486/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12866.9043 - accuracy: 0.7857 - val_loss: 6176.2383 - val_accuracy: 0.8571\n",
            "Epoch 487/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7424.0713 - accuracy: 0.8187 - val_loss: 863.2664 - val_accuracy: 0.7143\n",
            "Epoch 488/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10682.5684 - accuracy: 0.7637 - val_loss: 7234.8457 - val_accuracy: 0.8462\n",
            "Epoch 489/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3262.7869 - accuracy: 0.7967 - val_loss: 2665.3237 - val_accuracy: 0.8571\n",
            "Epoch 490/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5333.5469 - accuracy: 0.8297 - val_loss: 984.3214 - val_accuracy: 0.7143\n",
            "Epoch 491/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 13266.5869 - accuracy: 0.7637 - val_loss: 3110.5369 - val_accuracy: 0.8571\n",
            "Epoch 492/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9563.4502 - accuracy: 0.7830 - val_loss: 5949.4814 - val_accuracy: 0.8462\n",
            "Epoch 493/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5438.7622 - accuracy: 0.7885 - val_loss: 2796.4319 - val_accuracy: 0.8462\n",
            "Epoch 494/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4041.2734 - accuracy: 0.8242 - val_loss: 1291.3384 - val_accuracy: 0.7033\n",
            "Epoch 495/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2662.3662 - accuracy: 0.8187 - val_loss: 9159.1455 - val_accuracy: 0.8352\n",
            "Epoch 496/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 26505.9180 - accuracy: 0.8049 - val_loss: 20554.0723 - val_accuracy: 0.8242\n",
            "Epoch 497/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 20787.0117 - accuracy: 0.7253 - val_loss: 4413.4590 - val_accuracy: 0.5275\n",
            "Epoch 498/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 21131.1699 - accuracy: 0.7747 - val_loss: 7844.6758 - val_accuracy: 0.8352\n",
            "Epoch 499/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6072.9917 - accuracy: 0.7775 - val_loss: 14058.6025 - val_accuracy: 0.8352\n",
            "Epoch 500/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 11599.9619 - accuracy: 0.8324 - val_loss: 2918.4302 - val_accuracy: 0.6044\n",
            "Epoch 501/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 13603.8486 - accuracy: 0.7170 - val_loss: 11543.9756 - val_accuracy: 0.8352\n",
            "Epoch 502/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7118.6763 - accuracy: 0.8049 - val_loss: 1148.8416 - val_accuracy: 0.7033\n",
            "Epoch 503/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8780.2949 - accuracy: 0.8049 - val_loss: 6169.7632 - val_accuracy: 0.8462\n",
            "Epoch 504/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 13883.0166 - accuracy: 0.7418 - val_loss: 1905.5083 - val_accuracy: 0.8571\n",
            "Epoch 505/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10497.0088 - accuracy: 0.8379 - val_loss: 1718.1396 - val_accuracy: 0.6593\n",
            "Epoch 506/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10523.5938 - accuracy: 0.7692 - val_loss: 8420.7305 - val_accuracy: 0.8462\n",
            "Epoch 507/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10641.8643 - accuracy: 0.7582 - val_loss: 81.4416 - val_accuracy: 0.8681\n",
            "Epoch 508/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1447.5383 - accuracy: 0.8214 - val_loss: 591.9018 - val_accuracy: 0.7692\n",
            "Epoch 509/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3791.5061 - accuracy: 0.8242 - val_loss: 2431.0137 - val_accuracy: 0.7033\n",
            "Epoch 510/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8992.4102 - accuracy: 0.7527 - val_loss: 6098.2300 - val_accuracy: 0.8352\n",
            "Epoch 511/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4432.3638 - accuracy: 0.8297 - val_loss: 1810.7932 - val_accuracy: 0.8681\n",
            "Epoch 512/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1304.4448 - accuracy: 0.8516 - val_loss: 731.8640 - val_accuracy: 0.7363\n",
            "Epoch 513/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 12035.2119 - accuracy: 0.7720 - val_loss: 11224.3975 - val_accuracy: 0.8132\n",
            "Epoch 514/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8927.2334 - accuracy: 0.8132 - val_loss: 270.9738 - val_accuracy: 0.8571\n",
            "Epoch 515/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11465.9932 - accuracy: 0.8407 - val_loss: 1146.2822 - val_accuracy: 0.6923\n",
            "Epoch 516/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4320.6323 - accuracy: 0.7885 - val_loss: 1260.5078 - val_accuracy: 0.8571\n",
            "Epoch 517/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8434.5889 - accuracy: 0.8242 - val_loss: 2772.4814 - val_accuracy: 0.6484\n",
            "Epoch 518/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5109.2207 - accuracy: 0.7967 - val_loss: 100.3908 - val_accuracy: 0.8791\n",
            "Epoch 519/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 874.6220 - accuracy: 0.8434 - val_loss: 286.7241 - val_accuracy: 0.8791\n",
            "Epoch 520/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3167.8057 - accuracy: 0.8187 - val_loss: 5737.3662 - val_accuracy: 0.8352\n",
            "Epoch 521/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5671.9424 - accuracy: 0.7802 - val_loss: 628.0132 - val_accuracy: 0.7582\n",
            "Epoch 522/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2667.9377 - accuracy: 0.8214 - val_loss: 2656.0271 - val_accuracy: 0.6703\n",
            "Epoch 523/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10392.0225 - accuracy: 0.7940 - val_loss: 3458.5574 - val_accuracy: 0.8352\n",
            "Epoch 524/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1554.4729 - accuracy: 0.8516 - val_loss: 1161.3989 - val_accuracy: 0.7033\n",
            "Epoch 525/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7653.6553 - accuracy: 0.7830 - val_loss: 749.1807 - val_accuracy: 0.7253\n",
            "Epoch 526/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 22020.7832 - accuracy: 0.6813 - val_loss: 3620.9731 - val_accuracy: 0.8352\n",
            "Epoch 527/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 11419.9570 - accuracy: 0.8242 - val_loss: 1028.8239 - val_accuracy: 0.7033\n",
            "Epoch 528/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 8132.1440 - accuracy: 0.8104 - val_loss: 82.2774 - val_accuracy: 0.8571\n",
            "Epoch 529/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6153.4951 - accuracy: 0.7830 - val_loss: 562.4879 - val_accuracy: 0.7692\n",
            "Epoch 530/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1472.0045 - accuracy: 0.8132 - val_loss: 1863.7400 - val_accuracy: 0.8462\n",
            "Epoch 531/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1787.2704 - accuracy: 0.8681 - val_loss: 1386.3705 - val_accuracy: 0.6813\n",
            "Epoch 532/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 8264.5820 - accuracy: 0.8242 - val_loss: 1505.2521 - val_accuracy: 0.8681\n",
            "Epoch 533/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1543.0549 - accuracy: 0.8516 - val_loss: 2369.3379 - val_accuracy: 0.8681\n",
            "Epoch 534/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 5018.6479 - accuracy: 0.8379 - val_loss: 1373.2174 - val_accuracy: 0.8571\n",
            "Epoch 535/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 8126.7622 - accuracy: 0.7665 - val_loss: 14466.5703 - val_accuracy: 0.8132\n",
            "Epoch 536/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 24978.7656 - accuracy: 0.8132 - val_loss: 4821.1011 - val_accuracy: 0.8352\n",
            "Epoch 537/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 14242.1768 - accuracy: 0.6786 - val_loss: 1302.2444 - val_accuracy: 0.8462\n",
            "Epoch 538/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 8538.8535 - accuracy: 0.8489 - val_loss: 827.7900 - val_accuracy: 0.7143\n",
            "Epoch 539/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3127.6143 - accuracy: 0.8242 - val_loss: 896.2808 - val_accuracy: 0.6703\n",
            "Epoch 540/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 4180.2764 - accuracy: 0.8324 - val_loss: 414.8812 - val_accuracy: 0.7912\n",
            "Epoch 541/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2338.3430 - accuracy: 0.8187 - val_loss: 85.8849 - val_accuracy: 0.8681\n",
            "Epoch 542/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 7284.5293 - accuracy: 0.7830 - val_loss: 2865.6113 - val_accuracy: 0.8352\n",
            "Epoch 543/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2097.1775 - accuracy: 0.8379 - val_loss: 81.2785 - val_accuracy: 0.8462\n",
            "Epoch 544/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1735.3228 - accuracy: 0.7967 - val_loss: 224.1576 - val_accuracy: 0.8462\n",
            "Epoch 545/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 4102.5552 - accuracy: 0.8379 - val_loss: 1187.9543 - val_accuracy: 0.6813\n",
            "Epoch 546/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 10195.9219 - accuracy: 0.7418 - val_loss: 5572.9912 - val_accuracy: 0.8462\n",
            "Epoch 547/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 4566.4878 - accuracy: 0.8489 - val_loss: 418.9488 - val_accuracy: 0.8242\n",
            "Epoch 548/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 3258.7820 - accuracy: 0.7940 - val_loss: 1201.7445 - val_accuracy: 0.8681\n",
            "Epoch 549/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 3312.4248 - accuracy: 0.8077 - val_loss: 438.9807 - val_accuracy: 0.8132\n",
            "Epoch 550/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6383.7886 - accuracy: 0.8379 - val_loss: 505.3484 - val_accuracy: 0.8022\n",
            "Epoch 551/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 4925.3745 - accuracy: 0.8242 - val_loss: 1112.3130 - val_accuracy: 0.7143\n",
            "Epoch 552/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1788.8898 - accuracy: 0.8407 - val_loss: 290.4182 - val_accuracy: 0.8352\n",
            "Epoch 553/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1626.5447 - accuracy: 0.8516 - val_loss: 929.8347 - val_accuracy: 0.7033\n",
            "Epoch 554/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3191.4529 - accuracy: 0.8269 - val_loss: 145.0896 - val_accuracy: 0.8571\n",
            "Epoch 555/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2622.9573 - accuracy: 0.8654 - val_loss: 77.7637 - val_accuracy: 0.8571\n",
            "Epoch 556/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12627.0361 - accuracy: 0.7637 - val_loss: 8451.9023 - val_accuracy: 0.8462\n",
            "Epoch 557/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10726.5771 - accuracy: 0.7418 - val_loss: 3311.7410 - val_accuracy: 0.8462\n",
            "Epoch 558/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 19359.1328 - accuracy: 0.8269 - val_loss: 7627.7109 - val_accuracy: 0.8571\n",
            "Epoch 559/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6820.5688 - accuracy: 0.7995 - val_loss: 85.7350 - val_accuracy: 0.8681\n",
            "Epoch 560/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3983.9019 - accuracy: 0.7967 - val_loss: 5954.8975 - val_accuracy: 0.8242\n",
            "Epoch 561/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5360.8403 - accuracy: 0.8187 - val_loss: 6438.3525 - val_accuracy: 0.8462\n",
            "Epoch 562/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5692.0137 - accuracy: 0.8104 - val_loss: 2434.4783 - val_accuracy: 0.8571\n",
            "Epoch 563/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1400.7953 - accuracy: 0.8269 - val_loss: 1712.3732 - val_accuracy: 0.6484\n",
            "Epoch 564/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3005.3684 - accuracy: 0.8049 - val_loss: 5967.5825 - val_accuracy: 0.8242\n",
            "Epoch 565/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4700.1738 - accuracy: 0.8187 - val_loss: 336.6935 - val_accuracy: 0.8132\n",
            "Epoch 566/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2124.6497 - accuracy: 0.8132 - val_loss: 415.1994 - val_accuracy: 0.8571\n",
            "Epoch 567/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3433.3916 - accuracy: 0.8379 - val_loss: 5251.2329 - val_accuracy: 0.8242\n",
            "Epoch 568/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8936.9707 - accuracy: 0.7665 - val_loss: 290.9363 - val_accuracy: 0.8571\n",
            "Epoch 569/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2538.0447 - accuracy: 0.8077 - val_loss: 85.8043 - val_accuracy: 0.8462\n",
            "Epoch 570/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1128.4877 - accuracy: 0.8462 - val_loss: 5174.5698 - val_accuracy: 0.8462\n",
            "Epoch 571/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6256.5288 - accuracy: 0.8214 - val_loss: 1657.3945 - val_accuracy: 0.6923\n",
            "Epoch 572/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6269.2554 - accuracy: 0.8104 - val_loss: 1834.9587 - val_accuracy: 0.8462\n",
            "Epoch 573/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10959.9287 - accuracy: 0.8242 - val_loss: 81.0859 - val_accuracy: 0.8681\n",
            "Epoch 574/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3136.8069 - accuracy: 0.8077 - val_loss: 492.8399 - val_accuracy: 0.8571\n",
            "Epoch 575/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2234.8857 - accuracy: 0.8269 - val_loss: 768.4077 - val_accuracy: 0.8571\n",
            "Epoch 576/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2553.4312 - accuracy: 0.8049 - val_loss: 1346.0824 - val_accuracy: 0.6813\n",
            "Epoch 577/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2542.1182 - accuracy: 0.8462 - val_loss: 943.1302 - val_accuracy: 0.7143\n",
            "Epoch 578/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2333.5710 - accuracy: 0.8544 - val_loss: 347.4542 - val_accuracy: 0.8681\n",
            "Epoch 579/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 956.7224 - accuracy: 0.8791 - val_loss: 1038.2833 - val_accuracy: 0.7143\n",
            "Epoch 580/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6453.3882 - accuracy: 0.8159 - val_loss: 529.6181 - val_accuracy: 0.8462\n",
            "Epoch 581/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 11246.7852 - accuracy: 0.7582 - val_loss: 17379.2246 - val_accuracy: 0.8132\n",
            "Epoch 582/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 17769.6602 - accuracy: 0.7940 - val_loss: 2830.4697 - val_accuracy: 0.6044\n",
            "Epoch 583/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5620.5229 - accuracy: 0.7500 - val_loss: 12810.3877 - val_accuracy: 0.8242\n",
            "Epoch 584/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11315.3730 - accuracy: 0.8242 - val_loss: 3373.0837 - val_accuracy: 0.5824\n",
            "Epoch 585/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11207.5508 - accuracy: 0.7390 - val_loss: 3732.8792 - val_accuracy: 0.8352\n",
            "Epoch 586/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1838.4734 - accuracy: 0.8269 - val_loss: 1695.9426 - val_accuracy: 0.6593\n",
            "Epoch 587/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6122.2759 - accuracy: 0.8022 - val_loss: 1979.0396 - val_accuracy: 0.6813\n",
            "Epoch 588/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 9542.1084 - accuracy: 0.7555 - val_loss: 10231.3398 - val_accuracy: 0.8132\n",
            "Epoch 589/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5864.7480 - accuracy: 0.8462 - val_loss: 2203.1484 - val_accuracy: 0.6703\n",
            "Epoch 590/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 11284.0195 - accuracy: 0.7473 - val_loss: 2986.6418 - val_accuracy: 0.8352\n",
            "Epoch 591/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1926.0435 - accuracy: 0.8104 - val_loss: 3043.0457 - val_accuracy: 0.8352\n",
            "Epoch 592/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3041.2568 - accuracy: 0.8681 - val_loss: 1172.6064 - val_accuracy: 0.8681\n",
            "Epoch 593/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3188.5459 - accuracy: 0.8379 - val_loss: 3117.9841 - val_accuracy: 0.8571\n",
            "Epoch 594/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1585.3346 - accuracy: 0.8269 - val_loss: 354.6362 - val_accuracy: 0.8242\n",
            "Epoch 595/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 924.2509 - accuracy: 0.8571 - val_loss: 1029.7432 - val_accuracy: 0.8681\n",
            "Epoch 596/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 532.3512 - accuracy: 0.8764 - val_loss: 284.9302 - val_accuracy: 0.8352\n",
            "Epoch 597/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 722.3873 - accuracy: 0.8709 - val_loss: 80.0801 - val_accuracy: 0.8791\n",
            "Epoch 598/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1303.3416 - accuracy: 0.8626 - val_loss: 72.8261 - val_accuracy: 0.8791\n",
            "Epoch 599/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3066.2173 - accuracy: 0.8049 - val_loss: 216.3046 - val_accuracy: 0.8681\n",
            "Epoch 600/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2943.4458 - accuracy: 0.8379 - val_loss: 1515.6700 - val_accuracy: 0.6923\n",
            "Epoch 601/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6990.1279 - accuracy: 0.7940 - val_loss: 4742.2817 - val_accuracy: 0.8352\n",
            "Epoch 602/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2286.1418 - accuracy: 0.8352 - val_loss: 2336.0327 - val_accuracy: 0.8571\n",
            "Epoch 603/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 4401.4897 - accuracy: 0.7802 - val_loss: 439.5094 - val_accuracy: 0.8681\n",
            "Epoch 604/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3193.5229 - accuracy: 0.7885 - val_loss: 1055.9718 - val_accuracy: 0.7143\n",
            "Epoch 605/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4571.0942 - accuracy: 0.8434 - val_loss: 171.2398 - val_accuracy: 0.8681\n",
            "Epoch 606/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1968.0549 - accuracy: 0.8379 - val_loss: 95.5837 - val_accuracy: 0.8681\n",
            "Epoch 607/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1511.5991 - accuracy: 0.8929 - val_loss: 81.8489 - val_accuracy: 0.8681\n",
            "Epoch 608/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1853.5060 - accuracy: 0.8407 - val_loss: 269.9444 - val_accuracy: 0.8462\n",
            "Epoch 609/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1530.3108 - accuracy: 0.8379 - val_loss: 640.9279 - val_accuracy: 0.8571\n",
            "Epoch 610/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1081.6852 - accuracy: 0.8544 - val_loss: 286.6129 - val_accuracy: 0.8132\n",
            "Epoch 611/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1068.6643 - accuracy: 0.8352 - val_loss: 94.0656 - val_accuracy: 0.8571\n",
            "Epoch 612/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1971.9814 - accuracy: 0.8407 - val_loss: 139.4448 - val_accuracy: 0.8571\n",
            "Epoch 613/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3677.2280 - accuracy: 0.8681 - val_loss: 546.8140 - val_accuracy: 0.7582\n",
            "Epoch 614/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3560.6284 - accuracy: 0.8187 - val_loss: 1111.4230 - val_accuracy: 0.6923\n",
            "Epoch 615/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3235.1985 - accuracy: 0.8104 - val_loss: 5173.3545 - val_accuracy: 0.8571\n",
            "Epoch 616/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2219.1189 - accuracy: 0.8077 - val_loss: 386.5426 - val_accuracy: 0.8022\n",
            "Epoch 617/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1403.2225 - accuracy: 0.8022 - val_loss: 1456.3860 - val_accuracy: 0.8571\n",
            "Epoch 618/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1241.4347 - accuracy: 0.8599 - val_loss: 497.2124 - val_accuracy: 0.8462\n",
            "Epoch 619/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 787.9135 - accuracy: 0.8407 - val_loss: 661.5986 - val_accuracy: 0.7582\n",
            "Epoch 620/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4169.3569 - accuracy: 0.8434 - val_loss: 451.2964 - val_accuracy: 0.8242\n",
            "Epoch 621/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5115.8682 - accuracy: 0.8049 - val_loss: 1053.8124 - val_accuracy: 0.8462\n",
            "Epoch 622/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3008.8950 - accuracy: 0.7747 - val_loss: 3917.6941 - val_accuracy: 0.8352\n",
            "Epoch 623/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2443.9717 - accuracy: 0.8544 - val_loss: 141.6219 - val_accuracy: 0.8681\n",
            "Epoch 624/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2120.7297 - accuracy: 0.8379 - val_loss: 277.8477 - val_accuracy: 0.8132\n",
            "Epoch 625/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2788.8584 - accuracy: 0.8132 - val_loss: 1940.3314 - val_accuracy: 0.6703\n",
            "Epoch 626/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4451.8545 - accuracy: 0.7995 - val_loss: 113.8330 - val_accuracy: 0.8901\n",
            "Epoch 627/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 880.9033 - accuracy: 0.8929 - val_loss: 421.4337 - val_accuracy: 0.8352\n",
            "Epoch 628/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3496.5203 - accuracy: 0.8159 - val_loss: 149.2498 - val_accuracy: 0.8681\n",
            "Epoch 629/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2791.1646 - accuracy: 0.8571 - val_loss: 78.6255 - val_accuracy: 0.9011\n",
            "Epoch 630/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1516.2086 - accuracy: 0.8269 - val_loss: 588.2512 - val_accuracy: 0.7802\n",
            "Epoch 631/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4905.7349 - accuracy: 0.8214 - val_loss: 459.3589 - val_accuracy: 0.8132\n",
            "Epoch 632/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3207.5347 - accuracy: 0.7775 - val_loss: 843.1909 - val_accuracy: 0.7143\n",
            "Epoch 633/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2647.8499 - accuracy: 0.8407 - val_loss: 105.6515 - val_accuracy: 0.8901\n",
            "Epoch 634/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4469.4707 - accuracy: 0.8077 - val_loss: 4283.7886 - val_accuracy: 0.8352\n",
            "Epoch 635/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9928.4453 - accuracy: 0.8407 - val_loss: 171.9386 - val_accuracy: 0.8791\n",
            "Epoch 636/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7164.6475 - accuracy: 0.8104 - val_loss: 8577.4639 - val_accuracy: 0.8462\n",
            "Epoch 637/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4471.7090 - accuracy: 0.8132 - val_loss: 1330.8228 - val_accuracy: 0.6593\n",
            "Epoch 638/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4885.7300 - accuracy: 0.8077 - val_loss: 1606.5010 - val_accuracy: 0.6703\n",
            "Epoch 639/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7662.5488 - accuracy: 0.7775 - val_loss: 3864.6875 - val_accuracy: 0.8462\n",
            "Epoch 640/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2782.4929 - accuracy: 0.8242 - val_loss: 1099.3470 - val_accuracy: 0.8462\n",
            "Epoch 641/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 11846.4932 - accuracy: 0.8544 - val_loss: 1072.9700 - val_accuracy: 0.8571\n",
            "Epoch 642/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2775.5935 - accuracy: 0.7692 - val_loss: 4544.3359 - val_accuracy: 0.8352\n",
            "Epoch 643/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5784.0249 - accuracy: 0.8187 - val_loss: 1826.7926 - val_accuracy: 0.8462\n",
            "Epoch 644/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1052.8223 - accuracy: 0.8599 - val_loss: 127.5371 - val_accuracy: 0.8352\n",
            "Epoch 645/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1117.7524 - accuracy: 0.8626 - val_loss: 605.7208 - val_accuracy: 0.8571\n",
            "Epoch 646/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1391.3251 - accuracy: 0.8407 - val_loss: 504.7585 - val_accuracy: 0.8571\n",
            "Epoch 647/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4292.1973 - accuracy: 0.8049 - val_loss: 7044.0947 - val_accuracy: 0.8352\n",
            "Epoch 648/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10148.5312 - accuracy: 0.7995 - val_loss: 1796.4338 - val_accuracy: 0.6703\n",
            "Epoch 649/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4574.3428 - accuracy: 0.8049 - val_loss: 3026.8420 - val_accuracy: 0.8571\n",
            "Epoch 650/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4825.1025 - accuracy: 0.7912 - val_loss: 3812.1995 - val_accuracy: 0.8462\n",
            "Epoch 651/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5541.7988 - accuracy: 0.7830 - val_loss: 425.4846 - val_accuracy: 0.8242\n",
            "Epoch 652/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2596.4692 - accuracy: 0.8269 - val_loss: 1539.3925 - val_accuracy: 0.6813\n",
            "Epoch 653/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4360.0312 - accuracy: 0.8242 - val_loss: 220.1622 - val_accuracy: 0.8791\n",
            "Epoch 654/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 606.5025 - accuracy: 0.8791 - val_loss: 538.2015 - val_accuracy: 0.8681\n",
            "Epoch 655/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 780.7861 - accuracy: 0.8764 - val_loss: 175.8505 - val_accuracy: 0.8681\n",
            "Epoch 656/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 362.6828 - accuracy: 0.8764 - val_loss: 84.7155 - val_accuracy: 0.8681\n",
            "Epoch 657/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 308.0104 - accuracy: 0.8901 - val_loss: 490.8892 - val_accuracy: 0.8242\n",
            "Epoch 658/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5796.4448 - accuracy: 0.8324 - val_loss: 565.0828 - val_accuracy: 0.7802\n",
            "Epoch 659/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 13901.3311 - accuracy: 0.7253 - val_loss: 890.1801 - val_accuracy: 0.7253\n",
            "Epoch 660/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 8368.4521 - accuracy: 0.8242 - val_loss: 1965.9830 - val_accuracy: 0.8571\n",
            "Epoch 661/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6580.8052 - accuracy: 0.7665 - val_loss: 2890.1436 - val_accuracy: 0.8352\n",
            "Epoch 662/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 9929.8672 - accuracy: 0.8462 - val_loss: 273.4174 - val_accuracy: 0.8242\n",
            "Epoch 663/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 4503.8369 - accuracy: 0.7802 - val_loss: 2985.0237 - val_accuracy: 0.8462\n",
            "Epoch 664/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3970.7925 - accuracy: 0.7830 - val_loss: 526.9573 - val_accuracy: 0.7692\n",
            "Epoch 665/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1997.4587 - accuracy: 0.8352 - val_loss: 384.5201 - val_accuracy: 0.8132\n",
            "Epoch 666/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1230.8429 - accuracy: 0.8324 - val_loss: 1480.5594 - val_accuracy: 0.8571\n",
            "Epoch 667/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 5445.2153 - accuracy: 0.8407 - val_loss: 457.3824 - val_accuracy: 0.8681\n",
            "Epoch 668/1000\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1642.1249 - accuracy: 0.8407 - val_loss: 178.8808 - val_accuracy: 0.8681\n",
            "Epoch 669/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1452.8337 - accuracy: 0.8462 - val_loss: 212.9118 - val_accuracy: 0.8571\n",
            "Epoch 670/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 4970.4829 - accuracy: 0.8269 - val_loss: 423.8951 - val_accuracy: 0.8681\n",
            "Epoch 671/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2132.1233 - accuracy: 0.8297 - val_loss: 371.9989 - val_accuracy: 0.8352\n",
            "Epoch 672/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 3098.2485 - accuracy: 0.8269 - val_loss: 741.1646 - val_accuracy: 0.7582\n",
            "Epoch 673/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 3422.4597 - accuracy: 0.8434 - val_loss: 273.8827 - val_accuracy: 0.8791\n",
            "Epoch 674/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2385.3870 - accuracy: 0.8242 - val_loss: 743.6983 - val_accuracy: 0.7582\n",
            "Epoch 675/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1242.2260 - accuracy: 0.8352 - val_loss: 97.3598 - val_accuracy: 0.8681\n",
            "Epoch 676/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 874.4141 - accuracy: 0.8874 - val_loss: 288.3264 - val_accuracy: 0.8681\n",
            "Epoch 677/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 3089.3381 - accuracy: 0.8764 - val_loss: 87.7961 - val_accuracy: 0.8791\n",
            "Epoch 678/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 922.2633 - accuracy: 0.8681 - val_loss: 561.4464 - val_accuracy: 0.8132\n",
            "Epoch 679/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1036.7567 - accuracy: 0.8599 - val_loss: 181.7568 - val_accuracy: 0.8681\n",
            "Epoch 680/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1011.3453 - accuracy: 0.8599 - val_loss: 84.4654 - val_accuracy: 0.8681\n",
            "Epoch 681/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2604.1658 - accuracy: 0.8242 - val_loss: 732.8469 - val_accuracy: 0.7582\n",
            "Epoch 682/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 7590.8730 - accuracy: 0.8324 - val_loss: 877.6309 - val_accuracy: 0.7363\n",
            "Epoch 683/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6090.7212 - accuracy: 0.7830 - val_loss: 8039.2212 - val_accuracy: 0.8352\n",
            "Epoch 684/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 7032.2671 - accuracy: 0.8544 - val_loss: 1123.3494 - val_accuracy: 0.6813\n",
            "Epoch 685/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7933.1587 - accuracy: 0.7720 - val_loss: 2167.6899 - val_accuracy: 0.8571\n",
            "Epoch 686/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2146.8267 - accuracy: 0.8104 - val_loss: 135.8710 - val_accuracy: 0.8571\n",
            "Epoch 687/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3425.3301 - accuracy: 0.8352 - val_loss: 111.8998 - val_accuracy: 0.8681\n",
            "Epoch 688/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 666.5903 - accuracy: 0.8874 - val_loss: 2338.5293 - val_accuracy: 0.8681\n",
            "Epoch 689/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2388.3176 - accuracy: 0.8379 - val_loss: 1569.1260 - val_accuracy: 0.8571\n",
            "Epoch 690/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1109.5435 - accuracy: 0.8544 - val_loss: 88.3248 - val_accuracy: 0.8791\n",
            "Epoch 691/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3536.0332 - accuracy: 0.8434 - val_loss: 193.8362 - val_accuracy: 0.8352\n",
            "Epoch 692/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 13565.4854 - accuracy: 0.7308 - val_loss: 999.7491 - val_accuracy: 0.8681\n",
            "Epoch 693/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 11797.6934 - accuracy: 0.8462 - val_loss: 2035.0536 - val_accuracy: 0.8681\n",
            "Epoch 694/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4215.5444 - accuracy: 0.7802 - val_loss: 2284.4863 - val_accuracy: 0.8681\n",
            "Epoch 695/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7926.7754 - accuracy: 0.8489 - val_loss: 2394.1265 - val_accuracy: 0.8462\n",
            "Epoch 696/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1532.4324 - accuracy: 0.8462 - val_loss: 2482.6873 - val_accuracy: 0.8462\n",
            "Epoch 697/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1375.6031 - accuracy: 0.8544 - val_loss: 217.2654 - val_accuracy: 0.8462\n",
            "Epoch 698/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 509.5157 - accuracy: 0.8599 - val_loss: 877.8558 - val_accuracy: 0.8571\n",
            "Epoch 699/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 941.2849 - accuracy: 0.8489 - val_loss: 1118.3005 - val_accuracy: 0.8571\n",
            "Epoch 700/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 661.7647 - accuracy: 0.8846 - val_loss: 203.4065 - val_accuracy: 0.8571\n",
            "Epoch 701/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 701.9738 - accuracy: 0.8681 - val_loss: 822.3939 - val_accuracy: 0.8681\n",
            "Epoch 702/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 989.6876 - accuracy: 0.8791 - val_loss: 76.8863 - val_accuracy: 0.8681\n",
            "Epoch 703/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2156.8557 - accuracy: 0.8159 - val_loss: 1841.1174 - val_accuracy: 0.8681\n",
            "Epoch 704/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5554.7900 - accuracy: 0.8462 - val_loss: 112.4373 - val_accuracy: 0.8571\n",
            "Epoch 705/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9662.1768 - accuracy: 0.7390 - val_loss: 3358.9060 - val_accuracy: 0.8462\n",
            "Epoch 706/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7910.2876 - accuracy: 0.8516 - val_loss: 735.3724 - val_accuracy: 0.7363\n",
            "Epoch 707/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6204.1499 - accuracy: 0.7857 - val_loss: 4185.9688 - val_accuracy: 0.8352\n",
            "Epoch 708/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4589.1470 - accuracy: 0.8297 - val_loss: 1144.1714 - val_accuracy: 0.8681\n",
            "Epoch 709/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3019.8887 - accuracy: 0.8242 - val_loss: 1109.2224 - val_accuracy: 0.7033\n",
            "Epoch 710/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2375.2056 - accuracy: 0.8242 - val_loss: 3647.8486 - val_accuracy: 0.8352\n",
            "Epoch 711/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2923.2095 - accuracy: 0.8434 - val_loss: 74.6171 - val_accuracy: 0.8681\n",
            "Epoch 712/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2688.5911 - accuracy: 0.8352 - val_loss: 1459.6213 - val_accuracy: 0.8681\n",
            "Epoch 713/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5156.3003 - accuracy: 0.8077 - val_loss: 1247.9006 - val_accuracy: 0.6813\n",
            "Epoch 714/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2833.3938 - accuracy: 0.8132 - val_loss: 5369.8604 - val_accuracy: 0.8462\n",
            "Epoch 715/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3679.3193 - accuracy: 0.8214 - val_loss: 1065.1088 - val_accuracy: 0.7033\n",
            "Epoch 716/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4129.0791 - accuracy: 0.8324 - val_loss: 482.8241 - val_accuracy: 0.8022\n",
            "Epoch 717/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1381.8862 - accuracy: 0.8407 - val_loss: 3061.1096 - val_accuracy: 0.8462\n",
            "Epoch 718/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2257.4465 - accuracy: 0.8379 - val_loss: 650.5839 - val_accuracy: 0.8571\n",
            "Epoch 719/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1038.7670 - accuracy: 0.8599 - val_loss: 373.8293 - val_accuracy: 0.8132\n",
            "Epoch 720/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1272.1350 - accuracy: 0.8407 - val_loss: 694.3488 - val_accuracy: 0.7802\n",
            "Epoch 721/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2098.9617 - accuracy: 0.8104 - val_loss: 117.2325 - val_accuracy: 0.8901\n",
            "Epoch 722/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 541.5010 - accuracy: 0.8846 - val_loss: 85.4215 - val_accuracy: 0.8901\n",
            "Epoch 723/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 696.7653 - accuracy: 0.8846 - val_loss: 121.0749 - val_accuracy: 0.8681\n",
            "Epoch 724/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 385.3771 - accuracy: 0.8764 - val_loss: 1102.4611 - val_accuracy: 0.8681\n",
            "Epoch 725/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1947.1858 - accuracy: 0.8297 - val_loss: 264.6796 - val_accuracy: 0.8352\n",
            "Epoch 726/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1642.3040 - accuracy: 0.8764 - val_loss: 760.6126 - val_accuracy: 0.7692\n",
            "Epoch 727/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5703.5918 - accuracy: 0.7720 - val_loss: 4929.3291 - val_accuracy: 0.8352\n",
            "Epoch 728/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3362.1282 - accuracy: 0.8681 - val_loss: 100.9877 - val_accuracy: 0.8681\n",
            "Epoch 729/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1212.8463 - accuracy: 0.8791 - val_loss: 1437.7133 - val_accuracy: 0.8462\n",
            "Epoch 730/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6293.6270 - accuracy: 0.7610 - val_loss: 1986.9215 - val_accuracy: 0.8681\n",
            "Epoch 731/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 20283.2754 - accuracy: 0.8269 - val_loss: 18208.6699 - val_accuracy: 0.8132\n",
            "Epoch 732/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10562.8525 - accuracy: 0.8434 - val_loss: 1076.3168 - val_accuracy: 0.6374\n",
            "Epoch 733/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2238.2578 - accuracy: 0.8077 - val_loss: 455.8122 - val_accuracy: 0.7363\n",
            "Epoch 734/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7041.1182 - accuracy: 0.7143 - val_loss: 1221.9467 - val_accuracy: 0.8352\n",
            "Epoch 735/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5117.1045 - accuracy: 0.8599 - val_loss: 693.4554 - val_accuracy: 0.8462\n",
            "Epoch 736/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2694.7129 - accuracy: 0.7775 - val_loss: 817.2175 - val_accuracy: 0.8462\n",
            "Epoch 737/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1487.8859 - accuracy: 0.8462 - val_loss: 97.4365 - val_accuracy: 0.8791\n",
            "Epoch 738/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1458.4071 - accuracy: 0.8379 - val_loss: 190.4938 - val_accuracy: 0.8791\n",
            "Epoch 739/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 185.0314 - accuracy: 0.9038 - val_loss: 80.3048 - val_accuracy: 0.8791\n",
            "Epoch 740/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 981.2230 - accuracy: 0.8819 - val_loss: 369.3180 - val_accuracy: 0.8352\n",
            "Epoch 741/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3610.0474 - accuracy: 0.8324 - val_loss: 999.2164 - val_accuracy: 0.8571\n",
            "Epoch 742/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1561.5254 - accuracy: 0.8077 - val_loss: 218.2188 - val_accuracy: 0.8681\n",
            "Epoch 743/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1070.5062 - accuracy: 0.8764 - val_loss: 73.3105 - val_accuracy: 0.8681\n",
            "Epoch 744/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 885.7233 - accuracy: 0.8571 - val_loss: 125.8352 - val_accuracy: 0.8681\n",
            "Epoch 745/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1883.9127 - accuracy: 0.8544 - val_loss: 102.1699 - val_accuracy: 0.8791\n",
            "Epoch 746/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 895.7291 - accuracy: 0.8791 - val_loss: 320.0003 - val_accuracy: 0.8791\n",
            "Epoch 747/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1329.7322 - accuracy: 0.8681 - val_loss: 129.3404 - val_accuracy: 0.8681\n",
            "Epoch 748/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2113.9158 - accuracy: 0.8764 - val_loss: 358.6368 - val_accuracy: 0.8352\n",
            "Epoch 749/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1564.4264 - accuracy: 0.8434 - val_loss: 808.6109 - val_accuracy: 0.8681\n",
            "Epoch 750/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1493.5276 - accuracy: 0.8462 - val_loss: 1201.6306 - val_accuracy: 0.8571\n",
            "Epoch 751/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3796.2839 - accuracy: 0.7912 - val_loss: 969.1803 - val_accuracy: 0.7143\n",
            "Epoch 752/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3137.6873 - accuracy: 0.8599 - val_loss: 222.7875 - val_accuracy: 0.8791\n",
            "Epoch 753/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3586.8835 - accuracy: 0.8516 - val_loss: 823.2839 - val_accuracy: 0.8681\n",
            "Epoch 754/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1745.2749 - accuracy: 0.8599 - val_loss: 184.7001 - val_accuracy: 0.8571\n",
            "Epoch 755/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 804.5733 - accuracy: 0.8489 - val_loss: 628.2266 - val_accuracy: 0.7582\n",
            "Epoch 756/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2531.8984 - accuracy: 0.8297 - val_loss: 543.8633 - val_accuracy: 0.8132\n",
            "Epoch 757/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9729.1260 - accuracy: 0.7610 - val_loss: 962.4850 - val_accuracy: 0.8681\n",
            "Epoch 758/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2612.7258 - accuracy: 0.8791 - val_loss: 712.6944 - val_accuracy: 0.8571\n",
            "Epoch 759/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1791.1667 - accuracy: 0.8654 - val_loss: 1237.6233 - val_accuracy: 0.8681\n",
            "Epoch 760/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5685.9209 - accuracy: 0.8489 - val_loss: 546.0344 - val_accuracy: 0.8571\n",
            "Epoch 761/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3594.3572 - accuracy: 0.7912 - val_loss: 1089.7455 - val_accuracy: 0.8352\n",
            "Epoch 762/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 887.4985 - accuracy: 0.8324 - val_loss: 263.6360 - val_accuracy: 0.8132\n",
            "Epoch 763/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 739.4399 - accuracy: 0.8736 - val_loss: 172.1680 - val_accuracy: 0.8571\n",
            "Epoch 764/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3140.9285 - accuracy: 0.8187 - val_loss: 347.8675 - val_accuracy: 0.8242\n",
            "Epoch 765/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 709.0256 - accuracy: 0.8544 - val_loss: 1636.8407 - val_accuracy: 0.8571\n",
            "Epoch 766/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2247.4175 - accuracy: 0.8626 - val_loss: 1061.3453 - val_accuracy: 0.8681\n",
            "Epoch 767/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1607.1173 - accuracy: 0.8516 - val_loss: 120.7227 - val_accuracy: 0.8571\n",
            "Epoch 768/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1471.8226 - accuracy: 0.8819 - val_loss: 485.8769 - val_accuracy: 0.7802\n",
            "Epoch 769/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4860.9404 - accuracy: 0.7637 - val_loss: 1462.6377 - val_accuracy: 0.8681\n",
            "Epoch 770/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3563.2703 - accuracy: 0.8736 - val_loss: 381.1006 - val_accuracy: 0.8352\n",
            "Epoch 771/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1532.9659 - accuracy: 0.8681 - val_loss: 68.1817 - val_accuracy: 0.8901\n",
            "Epoch 772/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 690.3778 - accuracy: 0.8626 - val_loss: 378.1942 - val_accuracy: 0.8352\n",
            "Epoch 773/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 603.3253 - accuracy: 0.8984 - val_loss: 102.6058 - val_accuracy: 0.8681\n",
            "Epoch 774/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 397.7360 - accuracy: 0.8874 - val_loss: 79.3290 - val_accuracy: 0.8681\n",
            "Epoch 775/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 376.1438 - accuracy: 0.8599 - val_loss: 87.3368 - val_accuracy: 0.8901\n",
            "Epoch 776/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 294.9362 - accuracy: 0.8984 - val_loss: 85.8789 - val_accuracy: 0.8791\n",
            "Epoch 777/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 353.2173 - accuracy: 0.8901 - val_loss: 239.3934 - val_accuracy: 0.8791\n",
            "Epoch 778/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 949.8406 - accuracy: 0.8764 - val_loss: 483.4344 - val_accuracy: 0.8242\n",
            "Epoch 779/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2422.1768 - accuracy: 0.8516 - val_loss: 186.5393 - val_accuracy: 0.8791\n",
            "Epoch 780/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 919.4251 - accuracy: 0.8764 - val_loss: 69.5847 - val_accuracy: 0.8791\n",
            "Epoch 781/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1599.5071 - accuracy: 0.8929 - val_loss: 164.6188 - val_accuracy: 0.8681\n",
            "Epoch 782/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1249.0079 - accuracy: 0.8434 - val_loss: 910.9536 - val_accuracy: 0.8462\n",
            "Epoch 783/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1736.8549 - accuracy: 0.8544 - val_loss: 1363.6281 - val_accuracy: 0.8462\n",
            "Epoch 784/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1734.7271 - accuracy: 0.8434 - val_loss: 728.5604 - val_accuracy: 0.8571\n",
            "Epoch 785/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1739.1680 - accuracy: 0.8269 - val_loss: 794.7914 - val_accuracy: 0.8462\n",
            "Epoch 786/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 523.4923 - accuracy: 0.9011 - val_loss: 60.8027 - val_accuracy: 0.8681\n",
            "Epoch 787/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 274.8385 - accuracy: 0.8819 - val_loss: 247.9841 - val_accuracy: 0.8352\n",
            "Epoch 788/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1052.5378 - accuracy: 0.8544 - val_loss: 71.4112 - val_accuracy: 0.8791\n",
            "Epoch 789/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 994.2079 - accuracy: 0.8571 - val_loss: 3275.0762 - val_accuracy: 0.8462\n",
            "Epoch 790/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 2531.1001 - accuracy: 0.8516 - val_loss: 714.9072 - val_accuracy: 0.7582\n",
            "Epoch 791/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3266.6895 - accuracy: 0.7830 - val_loss: 2440.1675 - val_accuracy: 0.8681\n",
            "Epoch 792/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1430.1753 - accuracy: 0.8297 - val_loss: 531.3217 - val_accuracy: 0.8681\n",
            "Epoch 793/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1598.2428 - accuracy: 0.8599 - val_loss: 106.1049 - val_accuracy: 0.8681\n",
            "Epoch 794/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 5399.8408 - accuracy: 0.8544 - val_loss: 3266.5801 - val_accuracy: 0.8571\n",
            "Epoch 795/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 7513.8755 - accuracy: 0.7665 - val_loss: 1285.4265 - val_accuracy: 0.6923\n",
            "Epoch 796/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1992.6959 - accuracy: 0.8132 - val_loss: 142.1748 - val_accuracy: 0.8681\n",
            "Epoch 797/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2746.2883 - accuracy: 0.8462 - val_loss: 1329.6270 - val_accuracy: 0.8681\n",
            "Epoch 798/1000\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 786.3678 - accuracy: 0.8736 - val_loss: 366.1714 - val_accuracy: 0.8791\n",
            "Epoch 799/1000\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1030.9844 - accuracy: 0.8654 - val_loss: 141.7447 - val_accuracy: 0.8791\n",
            "Epoch 800/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 980.6904 - accuracy: 0.8544 - val_loss: 453.0829 - val_accuracy: 0.8352\n",
            "Epoch 801/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2302.4448 - accuracy: 0.8544 - val_loss: 4089.0178 - val_accuracy: 0.8352\n",
            "Epoch 802/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3188.2239 - accuracy: 0.8297 - val_loss: 460.5146 - val_accuracy: 0.8132\n",
            "Epoch 803/1000\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 738.6809 - accuracy: 0.8819 - val_loss: 117.9055 - val_accuracy: 0.8791\n",
            "Epoch 804/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 902.8173 - accuracy: 0.8819 - val_loss: 343.6451 - val_accuracy: 0.8462\n",
            "Epoch 805/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2657.9131 - accuracy: 0.8132 - val_loss: 337.0678 - val_accuracy: 0.8681\n",
            "Epoch 806/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 401.1753 - accuracy: 0.8846 - val_loss: 58.1976 - val_accuracy: 0.8791\n",
            "Epoch 807/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1537.5386 - accuracy: 0.8462 - val_loss: 1437.2662 - val_accuracy: 0.8571\n",
            "Epoch 808/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1622.6250 - accuracy: 0.8626 - val_loss: 384.2440 - val_accuracy: 0.8681\n",
            "Epoch 809/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1332.4193 - accuracy: 0.8571 - val_loss: 148.3378 - val_accuracy: 0.8681\n",
            "Epoch 810/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 571.8217 - accuracy: 0.8874 - val_loss: 232.1062 - val_accuracy: 0.8462\n",
            "Epoch 811/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 954.6442 - accuracy: 0.8791 - val_loss: 346.0707 - val_accuracy: 0.8352\n",
            "Epoch 812/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3461.5449 - accuracy: 0.7747 - val_loss: 1367.9723 - val_accuracy: 0.8571\n",
            "Epoch 813/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3799.7625 - accuracy: 0.8571 - val_loss: 298.4608 - val_accuracy: 0.8352\n",
            "Epoch 814/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 2346.7061 - accuracy: 0.7995 - val_loss: 1936.3032 - val_accuracy: 0.8571\n",
            "Epoch 815/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1521.0404 - accuracy: 0.8736 - val_loss: 54.7319 - val_accuracy: 0.8791\n",
            "Epoch 816/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 140.7844 - accuracy: 0.8791 - val_loss: 394.7869 - val_accuracy: 0.8571\n",
            "Epoch 817/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 762.9175 - accuracy: 0.8489 - val_loss: 1226.5270 - val_accuracy: 0.7033\n",
            "Epoch 818/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4959.3242 - accuracy: 0.7830 - val_loss: 1766.8944 - val_accuracy: 0.8681\n",
            "Epoch 819/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1448.6205 - accuracy: 0.8846 - val_loss: 370.0515 - val_accuracy: 0.8462\n",
            "Epoch 820/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1807.2860 - accuracy: 0.8626 - val_loss: 128.0946 - val_accuracy: 0.8681\n",
            "Epoch 821/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 282.0559 - accuracy: 0.8846 - val_loss: 50.5069 - val_accuracy: 0.8901\n",
            "Epoch 822/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 385.9139 - accuracy: 0.8874 - val_loss: 53.3797 - val_accuracy: 0.8901\n",
            "Epoch 823/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 1475.7212 - accuracy: 0.8681 - val_loss: 326.5469 - val_accuracy: 0.8462\n",
            "Epoch 824/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2823.0027 - accuracy: 0.8654 - val_loss: 91.2417 - val_accuracy: 0.8681\n",
            "Epoch 825/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1747.0062 - accuracy: 0.8132 - val_loss: 1712.0874 - val_accuracy: 0.8681\n",
            "Epoch 826/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1709.2316 - accuracy: 0.8407 - val_loss: 72.2570 - val_accuracy: 0.8681\n",
            "Epoch 827/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2029.5208 - accuracy: 0.8736 - val_loss: 429.6612 - val_accuracy: 0.8022\n",
            "Epoch 828/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1698.1901 - accuracy: 0.8104 - val_loss: 2057.2537 - val_accuracy: 0.8681\n",
            "Epoch 829/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2178.8967 - accuracy: 0.8489 - val_loss: 387.4633 - val_accuracy: 0.8242\n",
            "Epoch 830/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1538.6732 - accuracy: 0.8242 - val_loss: 297.5027 - val_accuracy: 0.8352\n",
            "Epoch 831/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 680.7103 - accuracy: 0.8599 - val_loss: 129.4115 - val_accuracy: 0.8681\n",
            "Epoch 832/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 993.5332 - accuracy: 0.8599 - val_loss: 817.0009 - val_accuracy: 0.8571\n",
            "Epoch 833/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 395.1627 - accuracy: 0.8846 - val_loss: 754.8771 - val_accuracy: 0.8681\n",
            "Epoch 834/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 573.4039 - accuracy: 0.8764 - val_loss: 371.8283 - val_accuracy: 0.8352\n",
            "Epoch 835/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1064.6584 - accuracy: 0.8736 - val_loss: 105.1933 - val_accuracy: 0.8791\n",
            "Epoch 836/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 539.0830 - accuracy: 0.8819 - val_loss: 63.3472 - val_accuracy: 0.8901\n",
            "Epoch 837/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 274.5595 - accuracy: 0.8791 - val_loss: 127.1905 - val_accuracy: 0.8571\n",
            "Epoch 838/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 290.6367 - accuracy: 0.8736 - val_loss: 89.2181 - val_accuracy: 0.8791\n",
            "Epoch 839/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 309.8891 - accuracy: 0.8791 - val_loss: 123.8408 - val_accuracy: 0.8791\n",
            "Epoch 840/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 262.6256 - accuracy: 0.9038 - val_loss: 62.5770 - val_accuracy: 0.8791\n",
            "Epoch 841/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 427.1920 - accuracy: 0.8956 - val_loss: 63.4502 - val_accuracy: 0.8681\n",
            "Epoch 842/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 185.9247 - accuracy: 0.8901 - val_loss: 59.9077 - val_accuracy: 0.8791\n",
            "Epoch 843/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 266.8403 - accuracy: 0.8846 - val_loss: 50.3139 - val_accuracy: 0.8791\n",
            "Epoch 844/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 213.1879 - accuracy: 0.8929 - val_loss: 63.1261 - val_accuracy: 0.8901\n",
            "Epoch 845/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 461.8519 - accuracy: 0.8901 - val_loss: 67.7852 - val_accuracy: 0.8681\n",
            "Epoch 846/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 598.1242 - accuracy: 0.8599 - val_loss: 53.5670 - val_accuracy: 0.8901\n",
            "Epoch 847/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 233.1127 - accuracy: 0.8956 - val_loss: 52.5143 - val_accuracy: 0.8681\n",
            "Epoch 848/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1493.9880 - accuracy: 0.8462 - val_loss: 1890.8569 - val_accuracy: 0.8571\n",
            "Epoch 849/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 976.6759 - accuracy: 0.8874 - val_loss: 95.0811 - val_accuracy: 0.8681\n",
            "Epoch 850/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 656.3582 - accuracy: 0.8819 - val_loss: 114.9745 - val_accuracy: 0.8681\n",
            "Epoch 851/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1086.1732 - accuracy: 0.8379 - val_loss: 52.4241 - val_accuracy: 0.8901\n",
            "Epoch 852/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 843.7303 - accuracy: 0.8819 - val_loss: 54.5587 - val_accuracy: 0.8791\n",
            "Epoch 853/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 912.7415 - accuracy: 0.8791 - val_loss: 46.8526 - val_accuracy: 0.8681\n",
            "Epoch 854/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 685.6062 - accuracy: 0.8407 - val_loss: 255.1557 - val_accuracy: 0.8242\n",
            "Epoch 855/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 854.3127 - accuracy: 0.8489 - val_loss: 337.6737 - val_accuracy: 0.8242\n",
            "Epoch 856/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 913.1721 - accuracy: 0.8791 - val_loss: 126.2860 - val_accuracy: 0.8681\n",
            "Epoch 857/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 456.5497 - accuracy: 0.8764 - val_loss: 158.9439 - val_accuracy: 0.8681\n",
            "Epoch 858/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2819.1670 - accuracy: 0.8297 - val_loss: 1450.3970 - val_accuracy: 0.8571\n",
            "Epoch 859/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1737.2463 - accuracy: 0.8681 - val_loss: 136.3221 - val_accuracy: 0.8791\n",
            "Epoch 860/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 472.8854 - accuracy: 0.8929 - val_loss: 167.2635 - val_accuracy: 0.8462\n",
            "Epoch 861/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1894.4136 - accuracy: 0.8681 - val_loss: 53.3610 - val_accuracy: 0.8901\n",
            "Epoch 862/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 443.1840 - accuracy: 0.8242 - val_loss: 816.0923 - val_accuracy: 0.8571\n",
            "Epoch 863/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 862.6688 - accuracy: 0.8599 - val_loss: 69.4417 - val_accuracy: 0.8681\n",
            "Epoch 864/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1355.9978 - accuracy: 0.8352 - val_loss: 681.1124 - val_accuracy: 0.7253\n",
            "Epoch 865/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1891.9253 - accuracy: 0.8077 - val_loss: 1097.7087 - val_accuracy: 0.8571\n",
            "Epoch 866/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 660.4685 - accuracy: 0.8874 - val_loss: 60.7861 - val_accuracy: 0.8681\n",
            "Epoch 867/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 460.2201 - accuracy: 0.8874 - val_loss: 131.7158 - val_accuracy: 0.8681\n",
            "Epoch 868/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 334.5380 - accuracy: 0.8846 - val_loss: 151.7152 - val_accuracy: 0.8681\n",
            "Epoch 869/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 272.6686 - accuracy: 0.8819 - val_loss: 68.9944 - val_accuracy: 0.8791\n",
            "Epoch 870/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 804.6060 - accuracy: 0.8846 - val_loss: 74.8263 - val_accuracy: 0.8901\n",
            "Epoch 871/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1605.7665 - accuracy: 0.8077 - val_loss: 332.9195 - val_accuracy: 0.8791\n",
            "Epoch 872/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1760.3599 - accuracy: 0.8571 - val_loss: 279.6721 - val_accuracy: 0.8352\n",
            "Epoch 873/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 594.6028 - accuracy: 0.8901 - val_loss: 489.5153 - val_accuracy: 0.8681\n",
            "Epoch 874/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1613.9734 - accuracy: 0.8214 - val_loss: 833.3568 - val_accuracy: 0.8681\n",
            "Epoch 875/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2121.8542 - accuracy: 0.8654 - val_loss: 297.9340 - val_accuracy: 0.8242\n",
            "Epoch 876/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1006.8442 - accuracy: 0.8819 - val_loss: 240.4235 - val_accuracy: 0.8462\n",
            "Epoch 877/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 720.4508 - accuracy: 0.8516 - val_loss: 54.6108 - val_accuracy: 0.8791\n",
            "Epoch 878/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1038.3734 - accuracy: 0.8956 - val_loss: 132.3638 - val_accuracy: 0.8681\n",
            "Epoch 879/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 385.7946 - accuracy: 0.8709 - val_loss: 52.9216 - val_accuracy: 0.8791\n",
            "Epoch 880/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 122.2048 - accuracy: 0.8984 - val_loss: 96.3704 - val_accuracy: 0.8571\n",
            "Epoch 881/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 496.1584 - accuracy: 0.8764 - val_loss: 102.6721 - val_accuracy: 0.8791\n",
            "Epoch 882/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 444.0822 - accuracy: 0.8709 - val_loss: 1426.8549 - val_accuracy: 0.8571\n",
            "Epoch 883/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2499.0066 - accuracy: 0.8654 - val_loss: 331.4439 - val_accuracy: 0.8242\n",
            "Epoch 884/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2306.8359 - accuracy: 0.8132 - val_loss: 524.0043 - val_accuracy: 0.8462\n",
            "Epoch 885/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 737.4233 - accuracy: 0.8709 - val_loss: 51.0371 - val_accuracy: 0.8681\n",
            "Epoch 886/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 875.3347 - accuracy: 0.8571 - val_loss: 420.9118 - val_accuracy: 0.8462\n",
            "Epoch 887/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 914.6624 - accuracy: 0.8214 - val_loss: 610.0508 - val_accuracy: 0.8571\n",
            "Epoch 888/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1108.2101 - accuracy: 0.8379 - val_loss: 616.2175 - val_accuracy: 0.8571\n",
            "Epoch 889/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1195.4061 - accuracy: 0.8764 - val_loss: 391.6669 - val_accuracy: 0.8132\n",
            "Epoch 890/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1895.2325 - accuracy: 0.8434 - val_loss: 196.0413 - val_accuracy: 0.8462\n",
            "Epoch 891/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 832.6704 - accuracy: 0.8599 - val_loss: 305.3239 - val_accuracy: 0.8352\n",
            "Epoch 892/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1183.7301 - accuracy: 0.8489 - val_loss: 841.3227 - val_accuracy: 0.8571\n",
            "Epoch 893/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 866.3546 - accuracy: 0.8654 - val_loss: 602.3519 - val_accuracy: 0.8681\n",
            "Epoch 894/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1103.3795 - accuracy: 0.8709 - val_loss: 47.7459 - val_accuracy: 0.8681\n",
            "Epoch 895/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 286.4087 - accuracy: 0.8984 - val_loss: 50.9320 - val_accuracy: 0.8791\n",
            "Epoch 896/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 415.2903 - accuracy: 0.8709 - val_loss: 50.8974 - val_accuracy: 0.8791\n",
            "Epoch 897/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 593.6218 - accuracy: 0.8654 - val_loss: 1038.4930 - val_accuracy: 0.8681\n",
            "Epoch 898/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 931.8356 - accuracy: 0.8764 - val_loss: 353.0375 - val_accuracy: 0.8681\n",
            "Epoch 899/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1231.6891 - accuracy: 0.8681 - val_loss: 381.4628 - val_accuracy: 0.8132\n",
            "Epoch 900/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1060.0608 - accuracy: 0.8544 - val_loss: 1146.0759 - val_accuracy: 0.8571\n",
            "Epoch 901/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 829.4108 - accuracy: 0.8516 - val_loss: 49.7981 - val_accuracy: 0.8681\n",
            "Epoch 902/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 304.7904 - accuracy: 0.8764 - val_loss: 50.8944 - val_accuracy: 0.8681\n",
            "Epoch 903/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 381.8314 - accuracy: 0.8764 - val_loss: 203.5531 - val_accuracy: 0.8462\n",
            "Epoch 904/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 657.4777 - accuracy: 0.8764 - val_loss: 85.2268 - val_accuracy: 0.8571\n",
            "Epoch 905/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 766.2928 - accuracy: 0.8846 - val_loss: 272.0495 - val_accuracy: 0.8462\n",
            "Epoch 906/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 440.6933 - accuracy: 0.8791 - val_loss: 72.5027 - val_accuracy: 0.8681\n",
            "Epoch 907/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 76.9851 - accuracy: 0.9011 - val_loss: 57.4109 - val_accuracy: 0.8791\n",
            "Epoch 908/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 59.0188 - accuracy: 0.9203 - val_loss: 228.8524 - val_accuracy: 0.8681\n",
            "Epoch 909/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1895.0131 - accuracy: 0.8462 - val_loss: 60.3269 - val_accuracy: 0.8791\n",
            "Epoch 910/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 573.1691 - accuracy: 0.9038 - val_loss: 179.1074 - val_accuracy: 0.8681\n",
            "Epoch 911/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 621.4241 - accuracy: 0.8791 - val_loss: 54.3965 - val_accuracy: 0.8681\n",
            "Epoch 912/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 729.9172 - accuracy: 0.8599 - val_loss: 967.3814 - val_accuracy: 0.8571\n",
            "Epoch 913/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1389.7052 - accuracy: 0.8681 - val_loss: 264.9460 - val_accuracy: 0.8352\n",
            "Epoch 914/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3641.2559 - accuracy: 0.7582 - val_loss: 52.9819 - val_accuracy: 0.8571\n",
            "Epoch 915/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 956.6786 - accuracy: 0.8544 - val_loss: 137.1511 - val_accuracy: 0.8352\n",
            "Epoch 916/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 718.9232 - accuracy: 0.8626 - val_loss: 75.2400 - val_accuracy: 0.8791\n",
            "Epoch 917/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 541.5109 - accuracy: 0.8791 - val_loss: 74.2354 - val_accuracy: 0.8791\n",
            "Epoch 918/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 770.9559 - accuracy: 0.8764 - val_loss: 96.9110 - val_accuracy: 0.8681\n",
            "Epoch 919/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 603.6200 - accuracy: 0.8654 - val_loss: 55.3382 - val_accuracy: 0.8681\n",
            "Epoch 920/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 523.6565 - accuracy: 0.8764 - val_loss: 212.9575 - val_accuracy: 0.8681\n",
            "Epoch 921/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 352.6500 - accuracy: 0.8709 - val_loss: 104.1302 - val_accuracy: 0.8462\n",
            "Epoch 922/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 447.8399 - accuracy: 0.8736 - val_loss: 189.6326 - val_accuracy: 0.8681\n",
            "Epoch 923/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1191.8197 - accuracy: 0.8681 - val_loss: 149.6587 - val_accuracy: 0.8571\n",
            "Epoch 924/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1117.2966 - accuracy: 0.8681 - val_loss: 204.0728 - val_accuracy: 0.8681\n",
            "Epoch 925/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 532.8299 - accuracy: 0.8791 - val_loss: 297.6367 - val_accuracy: 0.8462\n",
            "Epoch 926/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 329.1804 - accuracy: 0.8681 - val_loss: 422.1786 - val_accuracy: 0.8571\n",
            "Epoch 927/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 527.7541 - accuracy: 0.8791 - val_loss: 73.5986 - val_accuracy: 0.8791\n",
            "Epoch 928/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 928.2825 - accuracy: 0.8654 - val_loss: 163.9856 - val_accuracy: 0.8352\n",
            "Epoch 929/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 825.3011 - accuracy: 0.8516 - val_loss: 528.2426 - val_accuracy: 0.8571\n",
            "Epoch 930/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 792.9644 - accuracy: 0.8297 - val_loss: 43.6150 - val_accuracy: 0.8571\n",
            "Epoch 931/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1273.5343 - accuracy: 0.8681 - val_loss: 48.9720 - val_accuracy: 0.8791\n",
            "Epoch 932/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 755.0087 - accuracy: 0.8242 - val_loss: 843.9198 - val_accuracy: 0.8571\n",
            "Epoch 933/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 851.0055 - accuracy: 0.8544 - val_loss: 42.5438 - val_accuracy: 0.8681\n",
            "Epoch 934/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 806.0059 - accuracy: 0.8489 - val_loss: 188.2847 - val_accuracy: 0.8242\n",
            "Epoch 935/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 705.5703 - accuracy: 0.8544 - val_loss: 60.5958 - val_accuracy: 0.8791\n",
            "Epoch 936/1000\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 910.0942 - accuracy: 0.8297 - val_loss: 886.4914 - val_accuracy: 0.8571\n",
            "Epoch 937/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1268.3440 - accuracy: 0.8599 - val_loss: 38.8571 - val_accuracy: 0.8681\n",
            "Epoch 938/1000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 873.3834 - accuracy: 0.8681 - val_loss: 214.8298 - val_accuracy: 0.8352\n",
            "Epoch 939/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 532.6035 - accuracy: 0.8736 - val_loss: 53.5432 - val_accuracy: 0.8791\n",
            "Epoch 940/1000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 786.4915 - accuracy: 0.8654 - val_loss: 2471.0239 - val_accuracy: 0.8022\n",
            "Epoch 941/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2165.5471 - accuracy: 0.8571 - val_loss: 204.6294 - val_accuracy: 0.8242\n",
            "Epoch 942/1000\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1739.7461 - accuracy: 0.8297 - val_loss: 660.3990 - val_accuracy: 0.8462\n",
            "Epoch 943/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 646.9308 - accuracy: 0.8407 - val_loss: 89.8373 - val_accuracy: 0.8352\n",
            "Epoch 944/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 187.0420 - accuracy: 0.8544 - val_loss: 49.0722 - val_accuracy: 0.8791\n",
            "Epoch 945/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 134.9813 - accuracy: 0.8846 - val_loss: 329.2918 - val_accuracy: 0.8681\n",
            "Epoch 946/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 343.3291 - accuracy: 0.8819 - val_loss: 40.9706 - val_accuracy: 0.8901\n",
            "Epoch 947/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 152.3073 - accuracy: 0.8846 - val_loss: 36.3762 - val_accuracy: 0.8681\n",
            "Epoch 948/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 103.1420 - accuracy: 0.9011 - val_loss: 43.4698 - val_accuracy: 0.8791\n",
            "Epoch 949/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 599.5652 - accuracy: 0.8462 - val_loss: 44.5534 - val_accuracy: 0.8681\n",
            "Epoch 950/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2562.4192 - accuracy: 0.8681 - val_loss: 806.2709 - val_accuracy: 0.8571\n",
            "Epoch 951/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2225.6921 - accuracy: 0.8159 - val_loss: 39.5856 - val_accuracy: 0.8791\n",
            "Epoch 952/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1609.0966 - accuracy: 0.8791 - val_loss: 463.3792 - val_accuracy: 0.8571\n",
            "Epoch 953/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 587.5159 - accuracy: 0.8709 - val_loss: 133.7655 - val_accuracy: 0.8681\n",
            "Epoch 954/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 258.8388 - accuracy: 0.8819 - val_loss: 43.6633 - val_accuracy: 0.8791\n",
            "Epoch 955/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 109.6652 - accuracy: 0.8819 - val_loss: 78.0315 - val_accuracy: 0.8571\n",
            "Epoch 956/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 164.5110 - accuracy: 0.8846 - val_loss: 218.4340 - val_accuracy: 0.8681\n",
            "Epoch 957/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 419.6519 - accuracy: 0.8736 - val_loss: 47.6694 - val_accuracy: 0.8791\n",
            "Epoch 958/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 209.1307 - accuracy: 0.8764 - val_loss: 36.3447 - val_accuracy: 0.8571\n",
            "Epoch 959/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 160.5907 - accuracy: 0.8764 - val_loss: 124.2834 - val_accuracy: 0.8352\n",
            "Epoch 960/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 341.5881 - accuracy: 0.8709 - val_loss: 48.6485 - val_accuracy: 0.8462\n",
            "Epoch 961/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 178.8685 - accuracy: 0.8846 - val_loss: 39.0195 - val_accuracy: 0.8242\n",
            "Epoch 962/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 734.2777 - accuracy: 0.8764 - val_loss: 89.6301 - val_accuracy: 0.8352\n",
            "Epoch 963/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 953.9677 - accuracy: 0.8352 - val_loss: 632.2867 - val_accuracy: 0.8132\n",
            "Epoch 964/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1157.3588 - accuracy: 0.8462 - val_loss: 37.1000 - val_accuracy: 0.8462\n",
            "Epoch 965/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 841.2760 - accuracy: 0.8324 - val_loss: 193.2788 - val_accuracy: 0.8352\n",
            "Epoch 966/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 615.9329 - accuracy: 0.8654 - val_loss: 153.6893 - val_accuracy: 0.8242\n",
            "Epoch 967/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1259.5059 - accuracy: 0.8214 - val_loss: 57.2411 - val_accuracy: 0.8352\n",
            "Epoch 968/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 428.2333 - accuracy: 0.8516 - val_loss: 556.9667 - val_accuracy: 0.8132\n",
            "Epoch 969/1000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 711.7458 - accuracy: 0.8654 - val_loss: 113.4242 - val_accuracy: 0.8352\n",
            "Epoch 970/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 482.5151 - accuracy: 0.8407 - val_loss: 150.5335 - val_accuracy: 0.8242\n",
            "Epoch 971/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 872.8267 - accuracy: 0.8599 - val_loss: 203.3023 - val_accuracy: 0.8022\n",
            "Epoch 972/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 788.9716 - accuracy: 0.8434 - val_loss: 38.6546 - val_accuracy: 0.8352\n",
            "Epoch 973/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 660.8532 - accuracy: 0.8434 - val_loss: 784.0503 - val_accuracy: 0.8132\n",
            "Epoch 974/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 619.7359 - accuracy: 0.8434 - val_loss: 225.6673 - val_accuracy: 0.8022\n",
            "Epoch 975/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 657.5233 - accuracy: 0.8434 - val_loss: 716.7426 - val_accuracy: 0.8132\n",
            "Epoch 976/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 553.4251 - accuracy: 0.8489 - val_loss: 214.9679 - val_accuracy: 0.8022\n",
            "Epoch 977/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1056.4399 - accuracy: 0.8297 - val_loss: 432.3014 - val_accuracy: 0.8132\n",
            "Epoch 978/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1001.8231 - accuracy: 0.8571 - val_loss: 220.0910 - val_accuracy: 0.8132\n",
            "Epoch 979/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 949.6011 - accuracy: 0.8132 - val_loss: 521.2617 - val_accuracy: 0.8132\n",
            "Epoch 980/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 619.8406 - accuracy: 0.8544 - val_loss: 162.4853 - val_accuracy: 0.8242\n",
            "Epoch 981/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 894.5391 - accuracy: 0.8571 - val_loss: 105.1213 - val_accuracy: 0.8462\n",
            "Epoch 982/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 486.3115 - accuracy: 0.8626 - val_loss: 28.1057 - val_accuracy: 0.8681\n",
            "Epoch 983/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 71.8057 - accuracy: 0.8709 - val_loss: 77.5692 - val_accuracy: 0.8352\n",
            "Epoch 984/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 174.1944 - accuracy: 0.8819 - val_loss: 29.0717 - val_accuracy: 0.8681\n",
            "Epoch 985/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 122.3447 - accuracy: 0.8654 - val_loss: 28.8793 - val_accuracy: 0.8791\n",
            "Epoch 986/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 85.6118 - accuracy: 0.8709 - val_loss: 28.9336 - val_accuracy: 0.8791\n",
            "Epoch 987/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 328.5604 - accuracy: 0.8846 - val_loss: 30.4324 - val_accuracy: 0.8681\n",
            "Epoch 988/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 79.3111 - accuracy: 0.9038 - val_loss: 29.5800 - val_accuracy: 0.8681\n",
            "Epoch 989/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 71.5994 - accuracy: 0.8901 - val_loss: 28.5956 - val_accuracy: 0.8791\n",
            "Epoch 990/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 135.7739 - accuracy: 0.8956 - val_loss: 265.0195 - val_accuracy: 0.8791\n",
            "Epoch 991/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 594.1279 - accuracy: 0.8819 - val_loss: 122.3785 - val_accuracy: 0.8571\n",
            "Epoch 992/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 212.0089 - accuracy: 0.8681 - val_loss: 29.5862 - val_accuracy: 0.8791\n",
            "Epoch 993/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 301.7046 - accuracy: 0.8626 - val_loss: 45.0488 - val_accuracy: 0.8901\n",
            "Epoch 994/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 364.5091 - accuracy: 0.8626 - val_loss: 47.5083 - val_accuracy: 0.8901\n",
            "Epoch 995/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 339.3171 - accuracy: 0.8901 - val_loss: 108.1343 - val_accuracy: 0.8681\n",
            "Epoch 996/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 477.6304 - accuracy: 0.8571 - val_loss: 457.6349 - val_accuracy: 0.8681\n",
            "Epoch 997/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 727.3748 - accuracy: 0.8819 - val_loss: 199.5469 - val_accuracy: 0.8352\n",
            "Epoch 998/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 962.6248 - accuracy: 0.8379 - val_loss: 679.6058 - val_accuracy: 0.8571\n",
            "Epoch 999/1000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 953.8082 - accuracy: 0.8544 - val_loss: 198.0809 - val_accuracy: 0.8352\n",
            "Epoch 1000/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 450.5379 - accuracy: 0.8819 - val_loss: 31.3047 - val_accuracy: 0.8681\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 40.0221 - accuracy: 0.9386\n",
            "Accuracy (CNN): 0.9385964870452881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN0eBd5_cH8O",
        "outputId": "ad89acae-cf08-49b4-b3b1-f1ba07110873"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cNKQ78S3eJHw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}